{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e090044a-f87f-459c-b795-86404de7dd69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import functools\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Helper Functions\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_norm_layer(norm_type='instance'):\n",
    "    \"\"\"Return a normalization layer\n",
    "\n",
    "    Parameters:\n",
    "        norm_type (str) -- the name of the normalization layer: batch | instance | none\n",
    "\n",
    "    For BatchNorm, we use learnable affine parameters and track running statistics (mean/stddev).\n",
    "    For InstanceNorm, we do not use learnable affine parameters. We do not track running statistics.\n",
    "    \"\"\"\n",
    "    if norm_type == 'batch':\n",
    "        norm_layer = functools.partial(nn.BatchNorm2d, affine=True, track_running_stats=True)\n",
    "    elif norm_type == 'instance':\n",
    "        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n",
    "    elif norm_type == 'none':\n",
    "        def norm_layer(x): return Identity()\n",
    "    else:\n",
    "        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n",
    "    return norm_layer\n",
    "\n",
    "\n",
    "def get_scheduler(optimizer, opt):\n",
    "    \"\"\"Return a learning rate scheduler\n",
    "\n",
    "    Parameters:\n",
    "        optimizer          -- the optimizer of the network\n",
    "        opt (option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions．　\n",
    "                              opt.lr_policy is the name of learning rate policy: linear | step | plateau | cosine\n",
    "\n",
    "    For 'linear', we keep the same learning rate for the first <opt.n_epochs> epochs\n",
    "    and linearly decay the rate to zero over the next <opt.n_epochs_decay> epochs.\n",
    "    For other schedulers (step, plateau, and cosine), we use the default PyTorch schedulers.\n",
    "    See https://pytorch.org/docs/stable/optim.html for more details.\n",
    "    \"\"\"\n",
    "    if opt.lr_policy == 'linear':\n",
    "        def lambda_rule(epoch):\n",
    "            lr_l = 1.0 - max(0, epoch + opt.epoch_count - opt.n_epochs) / float(opt.n_epochs_decay + 1)\n",
    "            return lr_l\n",
    "        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n",
    "    elif opt.lr_policy == 'step':\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\n",
    "    elif opt.lr_policy == 'plateau':\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\n",
    "    elif opt.lr_policy == 'cosine':\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=opt.n_epochs, eta_min=0)\n",
    "    else:\n",
    "        return NotImplementedError('learning rate policy [%s] is not implemented', opt.lr_policy)\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "def init_weights(net, init_type='normal', init_gain=0.02):\n",
    "    \"\"\"Initialize network weights.\n",
    "\n",
    "    Parameters:\n",
    "        net (network)   -- network to be initialized\n",
    "        init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
    "        init_gain (float)    -- scaling factor for normal, xavier and orthogonal.\n",
    "\n",
    "    We use 'normal' in the original pix2pix and CycleGAN paper. But xavier and kaiming might\n",
    "    work better for some applications. Feel free to try yourself.\n",
    "    \"\"\"\n",
    "    def init_func(m):  # define the initialization function\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "            if init_type == 'normal':\n",
    "                init.normal_(m.weight.data, 0.0, init_gain)\n",
    "            elif init_type == 'xavier':\n",
    "                init.xavier_normal_(m.weight.data, gain=init_gain)\n",
    "            elif init_type == 'kaiming':\n",
    "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "            elif init_type == 'orthogonal':\n",
    "                init.orthogonal_(m.weight.data, gain=init_gain)\n",
    "            else:\n",
    "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find('BatchNorm2d') != -1:  # BatchNorm Layer's weight is not a matrix; only normal distribution applies.\n",
    "            init.normal_(m.weight.data, 1.0, init_gain)\n",
    "            init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    print('initialize network with %s' % init_type)\n",
    "    net.apply(init_func)  # apply the initialization function <init_func>\n",
    "\n",
    "\n",
    "def init_net(net, init_type='normal', init_gain=0.02, gpu_ids=[]):\n",
    "    \"\"\"Initialize a network: 1. register CPU/GPU device (with multi-GPU support); 2. initialize the network weights\n",
    "    Parameters:\n",
    "        net (network)      -- the network to be initialized\n",
    "        init_type (str)    -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
    "        gain (float)       -- scaling factor for normal, xavier and orthogonal.\n",
    "        gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2\n",
    "\n",
    "    Return an initialized network.\n",
    "    \"\"\"\n",
    "    if len(gpu_ids) > 0:\n",
    "        assert(torch.cuda.is_available())\n",
    "        net.to(gpu_ids[0])\n",
    "        net = torch.nn.DataParallel(net, gpu_ids)  # multi-GPUs\n",
    "    init_weights(net, init_type, init_gain=init_gain)\n",
    "    return net\n",
    "\n",
    "\n",
    "def define_G(input_nc, output_nc, ngf, netG, norm='batch', use_dropout=False, init_type='normal', init_gain=0.02, gpu_ids=[]):\n",
    "    \"\"\"Create a generator\n",
    "\n",
    "    Parameters:\n",
    "        input_nc (int) -- the number of channels in input images\n",
    "        output_nc (int) -- the number of channels in output images\n",
    "        ngf (int) -- the number of filters in the last conv layer\n",
    "        netG (str) -- the architecture's name: resnet_9blocks | resnet_6blocks | unet_256 | unet_128\n",
    "        norm (str) -- the name of normalization layers used in the network: batch | instance | none\n",
    "        use_dropout (bool) -- if use dropout layers.\n",
    "        init_type (str)    -- the name of our initialization method.\n",
    "        init_gain (float)  -- scaling factor for normal, xavier and orthogonal.\n",
    "        gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2\n",
    "\n",
    "    Returns a generator\n",
    "\n",
    "    Our current implementation provides two types of generators:\n",
    "        U-Net: [unet_128] (for 128x128 input images) and [unet_256] (for 256x256 input images)\n",
    "        The original U-Net paper: https://arxiv.org/abs/1505.04597\n",
    "\n",
    "        Resnet-based generator: [resnet_6blocks] (with 6 Resnet blocks) and [resnet_9blocks] (with 9 Resnet blocks)\n",
    "        Resnet-based generator consists of several Resnet blocks between a few downsampling/upsampling operations.\n",
    "        We adapt Torch code from Justin Johnson's neural style transfer project (https://github.com/jcjohnson/fast-neural-style).\n",
    "\n",
    "\n",
    "    The generator has been initialized by <init_net>. It uses RELU for non-linearity.\n",
    "    \"\"\"\n",
    "    net = None\n",
    "    norm_layer = get_norm_layer(norm_type=norm)\n",
    "\n",
    "    if netG == 'resnet_9blocks':\n",
    "        net = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=9)\n",
    "    elif netG == 'resnet_6blocks':\n",
    "        net = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=6)\n",
    "    elif netG == 'unet_128':\n",
    "        net = UnetGenerator(input_nc, output_nc, 7, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n",
    "    elif netG == 'unet_256':\n",
    "        net = UnetGenerator(input_nc, output_nc, 8, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n",
    "    elif netG == 'attention_unet_32':\n",
    "        net = Attention_UnetGenerator(input_nc, output_nc, 5, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n",
    "    else:\n",
    "        raise NotImplementedError('Generator model name [%s] is not recognized' % netG)\n",
    "    return init_net(net, init_type, init_gain, gpu_ids)\n",
    "\n",
    "\n",
    "def define_D(input_nc, ndf, netD, n_layers_D=3, norm='batch', init_type='normal', init_gain=0.02, gpu_ids=[]):\n",
    "    \"\"\"Create a discriminator\n",
    "\n",
    "    Parameters:\n",
    "        input_nc (int)     -- the number of channels in input images\n",
    "        ndf (int)          -- the number of filters in the first conv layer\n",
    "        netD (str)         -- the architecture's name: basic | n_layers | pixel\n",
    "        n_layers_D (int)   -- the number of conv layers in the discriminator; effective when netD=='n_layers'\n",
    "        norm (str)         -- the type of normalization layers used in the network.\n",
    "        init_type (str)    -- the name of the initialization method.\n",
    "        init_gain (float)  -- scaling factor for normal, xavier and orthogonal.\n",
    "        gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2\n",
    "\n",
    "    Returns a discriminator\n",
    "\n",
    "    Our current implementation provides three types of discriminators:\n",
    "        [basic]: 'PatchGAN' classifier described in the original pix2pix paper.\n",
    "        It can classify whether 70×70 overlapping patches are real or fake.\n",
    "        Such a patch-level discriminator architecture has fewer parameters\n",
    "        than a full-image discriminator and can work on arbitrarily-sized images\n",
    "        in a fully convolutional fashion.\n",
    "\n",
    "        [n_layers]: With this mode, you can specify the number of conv layers in the discriminator\n",
    "        with the parameter <n_layers_D> (default=3 as used in [basic] (PatchGAN).)\n",
    "\n",
    "        [pixel]: 1x1 PixelGAN discriminator can classify whether a pixel is real or not.\n",
    "        It encourages greater color diversity but has no effect on spatial statistics.\n",
    "\n",
    "    The discriminator has been initialized by <init_net>. It uses Leakly RELU for non-linearity.\n",
    "    \"\"\"\n",
    "    net = None\n",
    "    norm_layer = get_norm_layer(norm_type=norm)\n",
    "\n",
    "    if netD == 'basic':  # default PatchGAN classifier\n",
    "        net = NLayerDiscriminator(input_nc, ndf, n_layers=3, norm_layer=norm_layer)\n",
    "    elif netD == 'n_layers':  # more options\n",
    "        net = NLayerDiscriminator(input_nc, ndf, n_layers_D, norm_layer=norm_layer)\n",
    "    elif netD == 'pixel':     # classify if each pixel is real or fake\n",
    "        net = PixelDiscriminator(input_nc, ndf, norm_layer=norm_layer)\n",
    "    elif netD == 'conv':\n",
    "        net = ConvDiscriminator(input_nc)\n",
    "    else:\n",
    "        raise NotImplementedError('Discriminator model name [%s] is not recognized' % netD)\n",
    "    return init_net(net, init_type, init_gain, gpu_ids)\n",
    "\n",
    "def frequency_division(src_img):\n",
    "    #input:src_img    type:tensor\n",
    "    #output:image_low,image_high    type:tensor\n",
    "    #get low frequency component and high frequency compinent of image\n",
    "    fft_src = torch.rfft( src_img, signal_ndim=2, onesided=False ) \n",
    "    fft_amp = fft_src[:,:,:,:,0]**2 + fft_src[:,:,:,:,1]**2\n",
    "    fft_amp = torch.sqrt(fft_amp)\n",
    "    fft_pha = torch.atan2( fft_src[:,:,:,:,1], fft_src[:,:,:,:,0] )\n",
    "\n",
    "    # replace the low frequency amplitude part of source with that from target\n",
    "    _, _, h, w = fft_amp.size()\n",
    "    amp_low = torch.zeros(fft_amp.size(), dtype=torch.float)\n",
    "    b = (  np.floor(np.amin((h,w))*0.1)  ).astype(int)     # get b\n",
    "    amp_low[:,:,0:b,0:b]     = fft_amp[:,:,0:b,0:b]      # top left\n",
    "    amp_low[:,:,0:b,w-b:w]   = fft_amp[:,:,0:b,w-b:w]    # top right\n",
    "    amp_low[:,:,h-b:h,0:b]   = fft_amp[:,:,h-b:h,0:b]    # bottom left\n",
    "    amp_low[:,:,h-b:h,w-b:w] = fft_amp[:,:,h-b:h,w-b:w]  # bottom right\n",
    "    amp_high = fft_amp - amp_low \n",
    "\n",
    "    # recompose fft of source\n",
    "    fft_low = torch.zeros( fft_src.size(), dtype=torch.float )\n",
    "    fft_high = torch.zeros( fft_src.size(), dtype=torch.float )\n",
    "    fft_low[:,:,:,:,0] = torch.cos(fft_pha) * amp_low\n",
    "    fft_low[:,:,:,:,1] = torch.sin(fft_pha) * amp_low\n",
    "    fft_high[:,:,:,:,0] = torch.cos(fft_pha) * amp_high\n",
    "    fft_high[:,:,:,:,1] = torch.sin(fft_pha) * amp_high\n",
    "\n",
    "    # get the recomposed image: source content, target style\n",
    "    _, _, imgH, imgW = src_img.size()\n",
    "    image_low = torch.irfft(fft_low, signal_ndim=2, onesided=False, signal_sizes=[imgH,imgW])\n",
    "    image_high = torch.irfft(fft_high, signal_ndim=2, onesided=False, signal_sizes=[imgH,imgW])\n",
    "    return image_low,image_high\n",
    "\n",
    "##############################################################################\n",
    "# Classes\n",
    "##############################################################################\n",
    "class GANLoss(nn.Module):\n",
    "    \"\"\"Define different GAN objectives.\n",
    "\n",
    "    The GANLoss class abstracts away the need to create the target label tensor\n",
    "    that has the same size as the input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gan_mode, target_real_label=1.0, target_fake_label=0.0):\n",
    "        \"\"\" Initialize the GANLoss class.\n",
    "\n",
    "        Parameters:\n",
    "            gan_mode (str) - - the type of GAN objective. It currently supports vanilla, lsgan, and wgangp.\n",
    "            target_real_label (bool) - - label for a real image\n",
    "            target_fake_label (bool) - - label of a fake image\n",
    "\n",
    "        Note: Do not use sigmoid as the last layer of Discriminator.\n",
    "        LSGAN needs no sigmoid. vanilla GANs will handle it with BCEWithLogitsLoss.\n",
    "        \"\"\"\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.register_buffer('real_label', torch.tensor(target_real_label))\n",
    "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n",
    "        self.gan_mode = gan_mode\n",
    "        if gan_mode == 'lsgan':\n",
    "            self.loss = nn.MSELoss()\n",
    "        elif gan_mode == 'vanilla':\n",
    "            self.loss = nn.BCEWithLogitsLoss()\n",
    "        elif gan_mode in ['wgangp']:\n",
    "            self.loss = None\n",
    "        else:\n",
    "            raise NotImplementedError('gan mode %s not implemented' % gan_mode)\n",
    "\n",
    "    def get_target_tensor(self, prediction, target_is_real):\n",
    "        \"\"\"Create label tensors with the same size as the input.\n",
    "\n",
    "        Parameters:\n",
    "            prediction (tensor) - - tpyically the prediction from a discriminator\n",
    "            target_is_real (bool) - - if the ground truth label is for real images or fake images\n",
    "\n",
    "        Returns:\n",
    "            A label tensor filled with ground truth label, and with the size of the input\n",
    "        \"\"\"\n",
    "\n",
    "        if target_is_real:\n",
    "            target_tensor = self.real_label\n",
    "        else:\n",
    "            target_tensor = self.fake_label\n",
    "        return target_tensor.expand_as(prediction)\n",
    "\n",
    "    def __call__(self, prediction, target_is_real):\n",
    "        \"\"\"Calculate loss given Discriminator's output and grount truth labels.\n",
    "\n",
    "        Parameters:\n",
    "            prediction (tensor) - - tpyically the prediction output from a discriminator\n",
    "            target_is_real (bool) - - if the ground truth label is for real images or fake images\n",
    "\n",
    "        Returns:\n",
    "            the calculated loss.\n",
    "        \"\"\"\n",
    "        if self.gan_mode in ['lsgan', 'vanilla']:\n",
    "            target_tensor = self.get_target_tensor(prediction, target_is_real)\n",
    "            loss = self.loss(prediction, target_tensor)\n",
    "        elif self.gan_mode == 'wgangp':\n",
    "            if target_is_real:\n",
    "                loss = -prediction.mean()\n",
    "            else:\n",
    "                loss = prediction.mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "def cal_gradient_penalty(netD, real_data, fake_data, device, type='mixed', constant=1.0, lambda_gp=10.0):\n",
    "    \"\"\"Calculate the gradient penalty loss, used in WGAN-GP paper https://arxiv.org/abs/1704.00028\n",
    "\n",
    "    Arguments:\n",
    "        netD (network)              -- discriminator network\n",
    "        real_data (tensor array)    -- real images\n",
    "        fake_data (tensor array)    -- generated images from the generator\n",
    "        device (str)                -- GPU / CPU: from torch.device('cuda:{}'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device('cpu')\n",
    "        type (str)                  -- if we mix real and fake data or not [real | fake | mixed].\n",
    "        constant (float)            -- the constant used in formula ( ||gradient||_2 - constant)^2\n",
    "        lambda_gp (float)           -- weight for this loss\n",
    "\n",
    "    Returns the gradient penalty loss\n",
    "    \"\"\"\n",
    "    if lambda_gp > 0.0:\n",
    "        if type == 'real':   # either use real images, fake images, or a linear interpolation of two.\n",
    "            interpolatesv = real_data\n",
    "        elif type == 'fake':\n",
    "            interpolatesv = fake_data\n",
    "        elif type == 'mixed':\n",
    "            alpha = torch.rand(real_data.shape[0], 1, device=device)\n",
    "            alpha = alpha.expand(real_data.shape[0], real_data.nelement() // real_data.shape[0]).contiguous().view(*real_data.shape)\n",
    "            interpolatesv = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "        else:\n",
    "            raise NotImplementedError('{} not implemented'.format(type))\n",
    "        interpolatesv.requires_grad_(True)\n",
    "        disc_interpolates = netD(interpolatesv)\n",
    "        gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolatesv,\n",
    "                                        grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n",
    "                                        create_graph=True, retain_graph=True, only_inputs=True)\n",
    "        gradients = gradients[0].view(real_data.size(0), -1)  # flat the data\n",
    "        gradient_penalty = (((gradients + 1e-16).norm(2, dim=1) - constant) ** 2).mean() * lambda_gp        # added eps\n",
    "        return gradient_penalty, gradients\n",
    "    else:\n",
    "        return 0.0, None\n",
    "\n",
    "\n",
    "class ResnetGenerator(nn.Module):\n",
    "    \"\"\"Resnet-based generator that consists of Resnet blocks between a few downsampling/upsampling operations.\n",
    "\n",
    "    We adapt Torch code and idea from Justin Johnson's neural style transfer project(https://github.com/jcjohnson/fast-neural-style)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, padding_type='reflect'):\n",
    "        \"\"\"Construct a Resnet-based generator\n",
    "\n",
    "        Parameters:\n",
    "            input_nc (int)      -- the number of channels in input images\n",
    "            output_nc (int)     -- the number of channels in output images\n",
    "            ngf (int)           -- the number of filters in the last conv layer\n",
    "            norm_layer          -- normalization layer\n",
    "            use_dropout (bool)  -- if use dropout layers\n",
    "            n_blocks (int)      -- the number of ResNet blocks\n",
    "            padding_type (str)  -- the name of padding layer in conv layers: reflect | replicate | zero\n",
    "        \"\"\"\n",
    "        assert(n_blocks >= 0)\n",
    "        super(ResnetGenerator, self).__init__()\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "        model = [nn.ReflectionPad2d(3),\n",
    "                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),\n",
    "                 norm_layer(ngf),\n",
    "                 nn.ReLU(True)]\n",
    "\n",
    "        n_downsampling = 2\n",
    "        for i in range(n_downsampling):  # add downsampling layers\n",
    "            mult = 2 ** i\n",
    "            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias),\n",
    "                      norm_layer(ngf * mult * 2),\n",
    "                      nn.ReLU(True)]\n",
    "\n",
    "        mult = 2 ** n_downsampling\n",
    "        for i in range(n_blocks):       # add ResNet blocks\n",
    "\n",
    "            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n",
    "\n",
    "        for i in range(n_downsampling):  # add upsampling layers\n",
    "            mult = 2 ** (n_downsampling - i)\n",
    "            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n",
    "                                         kernel_size=3, stride=2,\n",
    "                                         padding=1, output_padding=1,\n",
    "                                         bias=use_bias),\n",
    "                      norm_layer(int(ngf * mult / 2)),\n",
    "                      nn.ReLU(True)]\n",
    "        model += [nn.ReflectionPad2d(3)]\n",
    "        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n",
    "        model += [nn.Tanh()]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Standard forward\"\"\"\n",
    "        return self.model(input)\n",
    "\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    \"\"\"Define a Resnet block\"\"\"\n",
    "\n",
    "    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
    "        \"\"\"Initialize the Resnet block\n",
    "\n",
    "        A resnet block is a conv block with skip connections\n",
    "        We construct a conv block with build_conv_block function,\n",
    "        and implement skip connections in <forward> function.\n",
    "        Original Resnet paper: https://arxiv.org/pdf/1512.03385.pdf\n",
    "        \"\"\"\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n",
    "\n",
    "    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
    "        \"\"\"Construct a convolutional block.\n",
    "\n",
    "        Parameters:\n",
    "            dim (int)           -- the number of channels in the conv layer.\n",
    "            padding_type (str)  -- the name of padding layer: reflect | replicate | zero\n",
    "            norm_layer          -- normalization layer\n",
    "            use_dropout (bool)  -- if use dropout layers.\n",
    "            use_bias (bool)     -- if the conv layer uses bias or not\n",
    "\n",
    "        Returns a conv block (with a conv layer, a normalization layer, and a non-linearity layer (ReLU))\n",
    "        \"\"\"\n",
    "        conv_block = []\n",
    "        p = 0\n",
    "        if padding_type == 'reflect':\n",
    "            conv_block += [nn.ReflectionPad2d(1)]\n",
    "        elif padding_type == 'replicate':\n",
    "            conv_block += [nn.ReplicationPad2d(1)]\n",
    "        elif padding_type == 'zero':\n",
    "            p = 1\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim), nn.ReLU(True)]\n",
    "        if use_dropout:\n",
    "            conv_block += [nn.Dropout(0.5)]\n",
    "\n",
    "        p = 0\n",
    "        if padding_type == 'reflect':\n",
    "            conv_block += [nn.ReflectionPad2d(1)]\n",
    "        elif padding_type == 'replicate':\n",
    "            conv_block += [nn.ReplicationPad2d(1)]\n",
    "        elif padding_type == 'zero':\n",
    "            p = 1\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim)]\n",
    "\n",
    "        return nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function (with skip connections)\"\"\"\n",
    "        out = x + self.conv_block(x)  # add skip connections\n",
    "        return out\n",
    "\n",
    "\n",
    "class UnetGenerator(nn.Module):\n",
    "    \"\"\"Create a Unet-based generator\"\"\"\n",
    "\n",
    "    def __init__(self, input_nc, output_nc, num_downs, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        \"\"\"Construct a Unet generator\n",
    "        Parameters:\n",
    "            input_nc (int)  -- the number of channels in input images\n",
    "            output_nc (int) -- the number of channels in output images\n",
    "            num_downs (int) -- the number of downsamplings in UNet. For example, # if |num_downs| == 7,\n",
    "                                image of size 128x128 will become of size 1x1 # at the bottleneck\n",
    "            ngf (int)       -- the number of filters in the last conv layer\n",
    "            norm_layer      -- normalization layer\n",
    "\n",
    "        We construct the U-Net from the innermost layer to the outermost layer.\n",
    "        It is a recursive process.\n",
    "        \"\"\"\n",
    "        super(UnetGenerator, self).__init__()\n",
    "        # construct unet structure\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)  # add the innermost layer\n",
    "        for i in range(num_downs - 5):          # add intermediate layers with ngf * 8 filters\n",
    "            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n",
    "        # gradually reduce the number of filters from ngf * 8 to ngf\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        self.model = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)  # add the outermost layer\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Standard forward\"\"\"\n",
    "        return self.model(input)\n",
    "\n",
    "\n",
    "class UnetSkipConnectionBlock(nn.Module):\n",
    "    \"\"\"Defines the Unet submodule with skip connection.\n",
    "        X -------------------identity----------------------\n",
    "        |-- downsampling -- |submodule| -- upsampling --|\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, outer_nc, inner_nc, input_nc=None,\n",
    "                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        \"\"\"Construct a Unet submodule with skip connections.\n",
    "\n",
    "        Parameters:\n",
    "            outer_nc (int) -- the number of filters in the outer conv layer\n",
    "            inner_nc (int) -- the number of filters in the inner conv layer\n",
    "            input_nc (int) -- the number of channels in input images/features\n",
    "            submodule (UnetSkipConnectionBlock) -- previously defined submodules\n",
    "            outermost (bool)    -- if this module is the outermost module\n",
    "            innermost (bool)    -- if this module is the innermost module\n",
    "            norm_layer          -- normalization layer\n",
    "            use_dropout (bool)  -- if use dropout layers.\n",
    "        \"\"\"\n",
    "        super(UnetSkipConnectionBlock, self).__init__()\n",
    "        self.outermost = outermost\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "        if input_nc is None:\n",
    "            input_nc = outer_nc\n",
    "        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n",
    "                             stride=2, padding=1, bias=use_bias)\n",
    "        downrelu = nn.LeakyReLU(0.2, True)\n",
    "        downnorm = norm_layer(inner_nc)\n",
    "        uprelu = nn.ReLU(True)\n",
    "        upnorm = norm_layer(outer_nc)\n",
    "\n",
    "        if outermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1)\n",
    "            down = [downconv]\n",
    "            up = [uprelu, upconv, nn.Tanh()]\n",
    "            model = down + [submodule] + up\n",
    "        elif innermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, bias=use_bias)\n",
    "            down = [downrelu, downconv]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            model = down + up\n",
    "        else:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, bias=use_bias)\n",
    "            down = [downrelu, downconv, downnorm]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "\n",
    "            if use_dropout:\n",
    "                model = down + [submodule] + up + [nn.Dropout(0.5)]\n",
    "            else:\n",
    "                model = down + [submodule] + up\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.outermost:\n",
    "            return self.model(x)\n",
    "        else:   # add skip connections\n",
    "            return torch.cat([x, self.model(x)], 1)\n",
    "\n",
    "class Attention_UnetGenerator(nn.Module):\n",
    "    \"\"\"Create a Unet-based generator\"\"\"\n",
    "\n",
    "    def __init__(self, input_nc, output_nc, num_downs, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        \"\"\"Construct a Unet generator\n",
    "        Parameters:\n",
    "            input_nc (int)  -- the number of channels in input images\n",
    "            output_nc (int) -- the number of channels in output images\n",
    "            num_downs (int) -- the number of downsamplings in UNet. For example, # if |num_downs| == 7,\n",
    "                                image of size 128x128 will become of size 1x1 # at the bottleneck\n",
    "            ngf (int)       -- the number of filters in the last conv layer\n",
    "            norm_layer      -- normalization layer\n",
    "\n",
    "        We construct the U-Net from the innermost layer to the outermost layer.\n",
    "        It is a recursive process.\n",
    "        \"\"\"\n",
    "        super(Attention_UnetGenerator, self).__init__()\n",
    "        # construct unet structure\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)  # add the innermost layer\n",
    "        for i in range(num_downs - 5):          # add intermediate layers with ngf * 8 filters\n",
    "            unet_block = Attention_UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n",
    "        # gradually reduce the number of filters from ngf * 8 to ngf\n",
    "        unet_block = Attention_UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        unet_block = Attention_UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        unet_block = Attention_UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        self.model = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)  # add the outermost layer\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Standard forward\"\"\"\n",
    "        return self.model(input)\n",
    "\n",
    "\n",
    "class Attention_UnetSkipConnectionBlock(nn.Module):\n",
    "    \"\"\"Defines the Unet submodule with skip connection.\n",
    "        X -------------------identity----------------------\n",
    "        |-- downsampling -- |submodule| -- upsampling --|\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, outer_nc, inner_nc, input_nc=None,\n",
    "                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        \"\"\"Construct a Unet submodule with skip connections.\n",
    "\n",
    "        Parameters:\n",
    "            outer_nc (int) -- the number of filters in the outer conv layer\n",
    "            inner_nc (int) -- the number of filters in the inner conv layer\n",
    "            input_nc (int) -- the number of channels in input images/features\n",
    "            submodule (UnetSkipConnectionBlock) -- previously defined submodules\n",
    "            outermost (bool)    -- if this module is the outermost module\n",
    "            innermost (bool)    -- if this module is the innermost module\n",
    "            norm_layer          -- normalization layer\n",
    "            use_dropout (bool)  -- if use dropout layers.\n",
    "        \"\"\"\n",
    "        super(Attention_UnetSkipConnectionBlock, self).__init__()\n",
    "        self.outermost = outermost\n",
    "        self.innermost = innermost\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "        if input_nc is None:\n",
    "            input_nc = outer_nc\n",
    "        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n",
    "                             stride=2, padding=1, bias=use_bias)\n",
    "        downrelu = nn.LeakyReLU(0.2, False)\n",
    "        downnorm = norm_layer(inner_nc)\n",
    "        uprelu = nn.ReLU(False)\n",
    "        upnorm = norm_layer(outer_nc)\n",
    "\n",
    "        self.W = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=outer_nc, out_channels=outer_nc, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(outer_nc),\n",
    "        )\n",
    "        self.theta = nn.Conv2d(in_channels=outer_nc, out_channels=outer_nc,\n",
    "                          kernel_size=2, stride=2, padding=0, bias=False)\n",
    "        self.phi = nn.Conv2d(in_channels=inner_nc * 2, out_channels=outer_nc,\n",
    "                        kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.psi = nn.Conv2d(in_channels=outer_nc, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\n",
    "        if outermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1)\n",
    "            down = [downconv]\n",
    "            up = [uprelu, upconv, nn.Tanh()]\n",
    "            model = down + [submodule] + up\n",
    "        elif innermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, bias=use_bias)\n",
    "            down = [downrelu, downconv]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            model = down + up\n",
    "        else:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, bias=use_bias)\n",
    "            down = [downrelu, downconv, downnorm]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "\n",
    "            if use_dropout:\n",
    "                model = down + [submodule] + up + [nn.Dropout(0.5)]\n",
    "            else:\n",
    "                model = down + [submodule] + up\n",
    "            gating = down + [submodule]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "        self.gating = nn.Sequential(*gating)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.outermost:\n",
    "            return self.model(x)\n",
    "        elif self.innermost:   # add skip connections\n",
    "            return torch.cat([x, self.model(x)], 1)\n",
    "        else:\n",
    "            input_size = x.size()\n",
    "            # theta => (b, c, t, h, w) -> (b, i_c, t, h, w) -> (b, i_c, thw)\n",
    "            # phi   => (b, g_d) -> (b, i_c)\n",
    "            theta_x = self.theta(x)\n",
    "            theta_x_size = theta_x.size()\n",
    "            g = self.gating(x)\n",
    "            # g (b, c, t', h', w') -> phi_g (b, i_c, t', h', w')\n",
    "            #  Relu(theta_x + phi_g + bias) -> f = (b, i_c, thw) -> (b, i_c, t/s1, h/s2, w/s3)\n",
    "            phi_g = F.interpolate(self.phi(g), size=theta_x_size[2:], mode='bilinear')\n",
    "            frelu = F.relu(theta_x + phi_g, inplace=False)\n",
    "            #  psi^T * f -> (b, psi_i_c, t/s1, h/s2, w/s3)\n",
    "            sigm_psif = torch.sigmoid(self.psi(frelu))\n",
    "            # upsample the attentions and multiply\n",
    "            sigm_psi_f = F.interpolate(sigm_psif, size=input_size[2:], mode='bilinear')\n",
    "            y = sigm_psi_f.expand_as(x) * x\n",
    "            W_y = self.W(y)\n",
    "            return torch.cat([W_y, self.model(x)], 1)\n",
    "\n",
    "class ConvDiscriminator(nn.Module):\n",
    "    def __init__(self, input_nc):\n",
    "        super(ConvDiscriminator, self).__init__()\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "    def forward(self, input):\n",
    "        return  self.conv1_1(input)\n",
    "\n",
    "class NLayerDiscriminator(nn.Module):\n",
    "    \"\"\"Defines a PatchGAN discriminator\"\"\"\n",
    "\n",
    "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n",
    "        \"\"\"Construct a PatchGAN discriminator\n",
    "\n",
    "        Parameters:\n",
    "            input_nc (int)  -- the number of channels in input images\n",
    "            ndf (int)       -- the number of filters in the last conv layer\n",
    "            n_layers (int)  -- the number of conv layers in the discriminator\n",
    "            norm_layer      -- normalization layer\n",
    "        \"\"\"\n",
    "        super(NLayerDiscriminator, self).__init__()\n",
    "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "        kw = 4\n",
    "        padw = 1\n",
    "        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n",
    "        nf_mult = 1\n",
    "        nf_mult_prev = 1\n",
    "        for n in range(1, n_layers):  # gradually increase the number of filters\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2 ** n, 8)\n",
    "            sequence += [\n",
    "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
    "                norm_layer(ndf * nf_mult),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2 ** n_layers, 8)\n",
    "        sequence += [\n",
    "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
    "            norm_layer(ndf * nf_mult),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n",
    "        self.model = nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Standard forward.\"\"\"\n",
    "        return self.model(input)\n",
    "\n",
    "\n",
    "class PixelDiscriminator(nn.Module):\n",
    "    \"\"\"Defines a 1x1 PatchGAN discriminator (pixelGAN)\"\"\"\n",
    "\n",
    "    def __init__(self, input_nc, ndf=64, norm_layer=nn.BatchNorm2d):\n",
    "        \"\"\"Construct a 1x1 PatchGAN discriminator\n",
    "\n",
    "        Parameters:\n",
    "            input_nc (int)  -- the number of channels in input images\n",
    "            ndf (int)       -- the number of filters in the last conv layer\n",
    "            norm_layer      -- normalization layer\n",
    "        \"\"\"\n",
    "        super(PixelDiscriminator, self).__init__()\n",
    "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "        self.net = [\n",
    "            nn.Conv2d(input_nc, ndf, kernel_size=1, stride=1, padding=0),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(ndf, ndf * 2, kernel_size=1, stride=1, padding=0, bias=use_bias),\n",
    "            norm_layer(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(ndf * 2, 1, kernel_size=1, stride=1, padding=0, bias=use_bias)]\n",
    "\n",
    "        self.net = nn.Sequential(*self.net)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Standard forward.\"\"\"\n",
    "        return self.net(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6402117d-50f9-4e6b-bbb5-5bff5d8393ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 22\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"This package contains modules related to objective functions, optimizations, and network architectures.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mTo add a custom model class called 'dummy', you need to add a file called 'dummy_model.py' and define a subclass DummyModel inherited from BaseModel.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03mSee our template model class 'template_model.py' for more details.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_model_using_name\u001b[39m(model_name):\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Import the module \"models/[model_name]_model.py\".\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    In the file, the class called DatasetNameModel() will\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    be instantiated. It has to be a subclass of BaseModel,\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    and it is case-insensitive.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "\"\"\"This package contains modules related to objective functions, optimizations, and network architectures.\n",
    "\n",
    "To add a custom model class called 'dummy', you need to add a file called 'dummy_model.py' and define a subclass DummyModel inherited from BaseModel.\n",
    "You need to implement the following five functions:\n",
    "    -- <__init__>:                      initialize the class; first call BaseModel.__init__(self, opt).\n",
    "    -- <set_input>:                     unpack data from dataset and apply preprocessing.\n",
    "    -- <forward>:                       produce intermediate results.\n",
    "    -- <optimize_parameters>:           calculate loss, gradients, and update network weights.\n",
    "    -- <modify_commandline_options>:    (optionally) add model-specific options and set default options.\n",
    "\n",
    "In the function <__init__>, you need to define four lists:\n",
    "    -- self.loss_names (str list):          specify the training losses that you want to plot and save.\n",
    "    -- self.model_names (str list):         define networks used in our training.\n",
    "    -- self.visual_names (str list):        specify the images that you want to display and save.\n",
    "    -- self.optimizers (optimizer list):    define and initialize optimizers. You can define one optimizer for each network. If two networks are updated at the same time, you can use itertools.chain to group them. See cycle_gan_model.py for an usage.\n",
    "\n",
    "Now you can use the model class by specifying flag '--model dummy'.\n",
    "See our template model class 'template_model.py' for more details.\n",
    "\"\"\"\n",
    "\n",
    "import importlib\n",
    "from models.base_model import BaseModel\n",
    "\n",
    "\n",
    "def find_model_using_name(model_name):\n",
    "    \"\"\"Import the module \"models/[model_name]_model.py\".\n",
    "\n",
    "    In the file, the class called DatasetNameModel() will\n",
    "    be instantiated. It has to be a subclass of BaseModel,\n",
    "    and it is case-insensitive.\n",
    "    \"\"\"\n",
    "    model_filename = \"models.\" + model_name + \"_model\"\n",
    "    modellib = importlib.import_module(model_filename)\n",
    "    model = None\n",
    "    target_model_name = model_name.replace('_', '') + 'model'\n",
    "    for name, cls in modellib.__dict__.items():\n",
    "        if name.lower() == target_model_name.lower() \\\n",
    "           and issubclass(cls, BaseModel):\n",
    "            model = cls\n",
    "\n",
    "    if model is None:\n",
    "        print(\"In %s.py, there should be a subclass of BaseModel with class name that matches %s in lowercase.\" % (model_filename, target_model_name))\n",
    "        exit(0)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_option_setter(model_name):\n",
    "    \"\"\"Return the static method <modify_commandline_options> of the model class.\"\"\"\n",
    "    model_class = find_model_using_name(model_name)\n",
    "    return model_class.modify_commandline_options\n",
    "\n",
    "\n",
    "def create_model(opt):\n",
    "    \"\"\"Create a model given the option.\n",
    "\n",
    "    This function warps the class CustomDatasetDataLoader.\n",
    "    This is the main interface between this package and 'train.py'/'test.py'\n",
    "\n",
    "    Example:\n",
    "        >>> from models import create_model\n",
    "        >>> model = create_model(opt)\n",
    "    \"\"\"\n",
    "    model = find_model_using_name(opt.model)\n",
    "    instance = model(opt)\n",
    "    print(\"model [%s] was created\" % type(instance).__name__)\n",
    "    return instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82bc4640-9ee6-4e03-98fe-36cce73efc81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mabc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ABC, abstractmethod\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m networks\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseModel\u001b[39;00m(ABC):\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This class is an abstract base class (ABC) for models.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    To create a subclass, you need to implement the following five functions:\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m        -- <__init__>:                      initialize the class; first call BaseModel.__init__(self, opt).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m        -- <modify_commandline_options>:    (optionally) add model-specific options and set default options.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "from abc import ABC, abstractmethod\n",
    "from . import networks\n",
    "\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    \"\"\"This class is an abstract base class (ABC) for models.\n",
    "    To create a subclass, you need to implement the following five functions:\n",
    "        -- <__init__>:                      initialize the class; first call BaseModel.__init__(self, opt).\n",
    "        -- <set_input>:                     unpack data from dataset and apply preprocessing.\n",
    "        -- <forward>:                       produce intermediate results.\n",
    "        -- <optimize_parameters>:           calculate losses, gradients, and update network weights.\n",
    "        -- <modify_commandline_options>:    (optionally) add model-specific options and set default options.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        \"\"\"Initialize the BaseModel class.\n",
    "\n",
    "        Parameters:\n",
    "            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n",
    "\n",
    "        When creating your custom class, you need to implement your own initialization.\n",
    "        In this function, you should first call <BaseModel.__init__(self, opt)>\n",
    "        Then, you need to define four lists:\n",
    "            -- self.loss_names (str list):          specify the training losses that you want to plot and save.\n",
    "            -- self.model_names (str list):         define networks used in our training.\n",
    "            -- self.visual_names (str list):        specify the images that you want to display and save.\n",
    "            -- self.optimizers (optimizer list):    define and initialize optimizers. You can define one optimizer for each network. If two networks are updated at the same time, you can use itertools.chain to group them. See cycle_gan_model.py for an example.\n",
    "        \"\"\"\n",
    "        self.opt = opt\n",
    "        self.gpu_ids = opt.gpu_ids\n",
    "        self.isTrain = opt.isTrain\n",
    "        self.device = torch.device('cuda:{}'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device('cpu')  # get device name: CPU or GPU\n",
    "        self.save_dir = os.path.join(opt.checkpoint_dir, opt.name)  # save all the checkpoints to save_dir\n",
    "        if opt.preprocess != 'scale_width':  # with [scale_width], input images might have different sizes, which hurts the performance of cudnn.benchmark.\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "        self.loss_names = []\n",
    "        self.model_names = []\n",
    "        self.visual_names = []\n",
    "        self.optimizers = []\n",
    "        self.image_paths = []\n",
    "        self.metric = 0  # used for learning rate policy 'plateau'\n",
    "\n",
    "    @staticmethod\n",
    "    def modify_commandline_options(parser, is_train):\n",
    "        \"\"\"Add new model-specific options, and rewrite default values for existing options.\n",
    "\n",
    "        Parameters:\n",
    "            parser          -- original option parser\n",
    "            is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.\n",
    "\n",
    "        Returns:\n",
    "            the modified parser.\n",
    "        \"\"\"\n",
    "        return parser\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_input(self, input):\n",
    "        \"\"\"Unpack input data from the dataloader and perform necessary pre-processing steps.\n",
    "\n",
    "        Parameters:\n",
    "            input (dict): includes the data itself and its metadata information.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self):\n",
    "        \"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def optimize_parameters(self):\n",
    "        \"\"\"Calculate losses, gradients, and update network weights; called in every training iteration\"\"\"\n",
    "        pass\n",
    "\n",
    "    def setup(self, opt):\n",
    "        \"\"\"Load and print networks; create schedulers\n",
    "\n",
    "        Parameters:\n",
    "            opt (Option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions\n",
    "        \"\"\"\n",
    "        if self.isTrain:\n",
    "            self.schedulers = [networks.get_scheduler(optimizer, opt) for optimizer in self.optimizers]\n",
    "        if not self.isTrain or opt.continue_train:\n",
    "            load_suffix = 'iter_%d' % opt.load_iter if opt.load_iter > 0 else opt.epoch\n",
    "            self.load_networks(load_suffix)\n",
    "        self.print_networks(opt.verbose)\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"Make models eval mode during test time\"\"\"\n",
    "        for name in self.model_names:\n",
    "            if isinstance(name, str):\n",
    "                net = getattr(self, 'net' + name)\n",
    "                net.eval()\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"Forward function used in test time.\n",
    "\n",
    "        This function wraps <forward> function in no_grad() so we don't save intermediate steps for backprop\n",
    "        It also calls <compute_visuals> to produce additional visualization results\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.forward()\n",
    "            self.compute_visuals()\n",
    "\n",
    "    def compute_visuals(self):\n",
    "        \"\"\"Calculate additional output images for visdom and HTML visualization\"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_image_paths(self):\n",
    "        \"\"\" Return image paths that are used to load current data\"\"\"\n",
    "        return self.image_paths\n",
    "\n",
    "    def update_learning_rate(self):\n",
    "        \"\"\"Update learning rates for all the networks; called at the end of every epoch\"\"\"\n",
    "        old_lr = self.optimizers[0].param_groups[0]['lr']\n",
    "        for scheduler in self.schedulers:\n",
    "            if self.opt.lr_policy == 'plateau':\n",
    "                scheduler.step(self.metric)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "        lr = self.optimizers[0].param_groups[0]['lr']\n",
    "        print('learning rate %.7f -> %.7f' % (old_lr, lr))\n",
    "\n",
    "    def get_current_visuals(self):\n",
    "        \"\"\"Return visualization images. train.py will display these images with visdom, and save the images to a HTML\"\"\"\n",
    "        visual_ret = OrderedDict()\n",
    "        for name in self.visual_names:\n",
    "            if isinstance(name, str):\n",
    "                visual_ret[name] = getattr(self, name)\n",
    "        return visual_ret\n",
    "\n",
    "    def get_current_losses(self):\n",
    "        \"\"\"Return traning losses / errors. train.py will print out these errors on console, and save them to a file\"\"\"\n",
    "        errors_ret = OrderedDict()\n",
    "        for name in self.loss_names:\n",
    "            if isinstance(name, str):\n",
    "                errors_ret[name] = float(getattr(self, 'loss_' + name))  # float(...) works for both scalar tensor and float number\n",
    "        return errors_ret\n",
    "\n",
    "    def save_networks(self, epoch):\n",
    "        \"\"\"Save all the networks to the disk.\n",
    "\n",
    "        Parameters:\n",
    "            epoch (int) -- current epoch; used in the file name '%s_net_%s.pth' % (epoch, name)\n",
    "        \"\"\"\n",
    "        for name in self.model_names:\n",
    "            if isinstance(name, str):\n",
    "                save_filename = '%s_net_%s.pth' % (epoch, name)\n",
    "                save_path = os.path.join(self.save_dir, save_filename)\n",
    "                net = getattr(self, 'net' + name)\n",
    "\n",
    "                if len(self.gpu_ids) > 0 and torch.cuda.is_available():\n",
    "                    torch.save(net.module.cpu().state_dict(), save_path)\n",
    "                    net.cuda(self.gpu_ids[0])\n",
    "                else:\n",
    "                    torch.save(net.cpu().state_dict(), save_path)\n",
    "\n",
    "    def __patch_instance_norm_state_dict(self, state_dict, module, keys, i=0):\n",
    "        \"\"\"Fix InstanceNorm checkpoints incompatibility (prior to 0.4)\"\"\"\n",
    "        key = keys[i]\n",
    "        if i + 1 == len(keys):  # at the end, pointing to a parameter/buffer\n",
    "            if module.__class__.__name__.startswith('InstanceNorm') and \\\n",
    "                    (key == 'running_mean' or key == 'running_var'):\n",
    "                if getattr(module, key) is None:\n",
    "                    state_dict.pop('.'.join(keys))\n",
    "            if module.__class__.__name__.startswith('InstanceNorm') and \\\n",
    "               (key == 'num_batches_tracked'):\n",
    "                state_dict.pop('.'.join(keys))\n",
    "        else:\n",
    "            self.__patch_instance_norm_state_dict(state_dict, getattr(module, key), keys, i + 1)\n",
    "\n",
    "    def load_networks(self, epoch):\n",
    "        \"\"\"Load all the networks from the disk.\n",
    "\n",
    "        Parameters:\n",
    "            epoch (int) -- current epoch; used in the file name '%s_net_%s.pth' % (epoch, name)\n",
    "        \"\"\"\n",
    "        for name in self.model_names:\n",
    "            if isinstance(name, str):\n",
    "                load_filename = '%s_net_%s.pth' % (epoch, name)\n",
    "                load_path = os.path.join(self.save_dir, load_filename)\n",
    "                net = getattr(self, 'net' + name)\n",
    "                if isinstance(net, torch.nn.DataParallel):\n",
    "                    net = net.module\n",
    "                print('loading the model from %s' % load_path)\n",
    "                # if you are using PyTorch newer than 0.4 (e.g., built from\n",
    "                # GitHub source), you can remove str() on self.device\n",
    "                state_dict = torch.load(load_path, map_location=str(self.device))\n",
    "                if hasattr(state_dict, '_metadata'):\n",
    "                    del state_dict._metadata\n",
    "\n",
    "                # patch InstanceNorm checkpoints prior to 0.4\n",
    "                for key in list(state_dict.keys()):  # need to copy keys here because we mutate in loop\n",
    "                    self.__patch_instance_norm_state_dict(state_dict, net, key.split('.'))\n",
    "                net.load_state_dict(state_dict)\n",
    "\n",
    "    def print_networks(self, verbose):\n",
    "        \"\"\"Print the total number of parameters in the network and (if verbose) network architecture\n",
    "\n",
    "        Parameters:\n",
    "            verbose (bool) -- if verbose: print the network architecture\n",
    "        \"\"\"\n",
    "        print('---------- Networks initialized -------------')\n",
    "        for name in self.model_names:\n",
    "            if isinstance(name, str):\n",
    "                net = getattr(self, 'net' + name)\n",
    "                num_params = 0\n",
    "                for param in net.parameters():\n",
    "                    num_params += param.numel()\n",
    "                if verbose:\n",
    "                    print(net)\n",
    "                print('[Network %s] Total number of parameters : %.3f M' % (name, num_params / 1e6))\n",
    "        print('-----------------------------------------------')\n",
    "\n",
    "    def set_requires_grad(self, nets, requires_grad=False):\n",
    "        \"\"\"Set requies_grad=Fasle for all the networks to avoid unnecessary computations\n",
    "        Parameters:\n",
    "            nets (network list)   -- a list of networks\n",
    "            requires_grad (bool)  -- whether the networks require gradients or not\n",
    "        \"\"\"\n",
    "        if not isinstance(nets, list):\n",
    "            nets = [nets]\n",
    "        for net in nets:\n",
    "            if net is not None:\n",
    "                for param in net.parameters():\n",
    "                    param.requires_grad = requires_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b1d4e09-e6e5-45a5-bb77-b768c0bf45fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 22\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"This package contains modules related to objective functions, optimizations, and network architectures.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mTo add a custom model class called 'dummy', you need to add a file called 'dummy_model.py' and define a subclass DummyModel inherited from BaseModel.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03mSee our template model class 'template_model.py' for more details.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_model_using_name\u001b[39m(model_name):\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Import the module \"models/[model_name]_model.py\".\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    In the file, the class called DatasetNameModel() will\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    be instantiated. It has to be a subclass of BaseModel,\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    and it is case-insensitive.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "\"\"\"This package contains modules related to objective functions, optimizations, and network architectures.\n",
    "\n",
    "To add a custom model class called 'dummy', you need to add a file called 'dummy_model.py' and define a subclass DummyModel inherited from BaseModel.\n",
    "You need to implement the following five functions:\n",
    "    -- <__init__>:                      initialize the class; first call BaseModel.__init__(self, opt).\n",
    "    -- <set_input>:                     unpack data from dataset and apply preprocessing.\n",
    "    -- <forward>:                       produce intermediate results.\n",
    "    -- <optimize_parameters>:           calculate loss, gradients, and update network weights.\n",
    "    -- <modify_commandline_options>:    (optionally) add model-specific options and set default options.\n",
    "\n",
    "In the function <__init__>, you need to define four lists:\n",
    "    -- self.loss_names (str list):          specify the training losses that you want to plot and save.\n",
    "    -- self.model_names (str list):         define networks used in our training.\n",
    "    -- self.visual_names (str list):        specify the images that you want to display and save.\n",
    "    -- self.optimizers (optimizer list):    define and initialize optimizers. You can define one optimizer for each network. If two networks are updated at the same time, you can use itertools.chain to group them. See cycle_gan_model.py for an usage.\n",
    "\n",
    "Now you can use the model class by specifying flag '--model dummy'.\n",
    "See our template model class 'template_model.py' for more details.\n",
    "\"\"\"\n",
    "\n",
    "import importlib\n",
    "from models.base_model import BaseModel\n",
    "\n",
    "\n",
    "def find_model_using_name(model_name):\n",
    "    \"\"\"Import the module \"models/[model_name]_model.py\".\n",
    "\n",
    "    In the file, the class called DatasetNameModel() will\n",
    "    be instantiated. It has to be a subclass of BaseModel,\n",
    "    and it is case-insensitive.\n",
    "    \"\"\"\n",
    "    model_filename = \"models.\" + model_name + \"_model\"\n",
    "    modellib = importlib.import_module(model_filename)\n",
    "    model = None\n",
    "    target_model_name = model_name.replace('_', '') + 'model'\n",
    "    for name, cls in modellib.__dict__.items():\n",
    "        if name.lower() == target_model_name.lower() \\\n",
    "           and issubclass(cls, BaseModel):\n",
    "            model = cls\n",
    "\n",
    "    if model is None:\n",
    "        print(\"In %s.py, there should be a subclass of BaseModel with class name that matches %s in lowercase.\" % (model_filename, target_model_name))\n",
    "        exit(0)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_option_setter(model_name):\n",
    "    \"\"\"Return the static method <modify_commandline_options> of the model class.\"\"\"\n",
    "    model_class = find_model_using_name(model_name)\n",
    "    return model_class.modify_commandline_options\n",
    "\n",
    "\n",
    "def create_model(opt):\n",
    "    \"\"\"Create a model given the option.\n",
    "\n",
    "    This function warps the class CustomDatasetDataLoader.\n",
    "    This is the main interface between this package and 'train.py'/'test.py'\n",
    "\n",
    "    Example:\n",
    "        >>> from models import create_model\n",
    "        >>> model = create_model(opt)\n",
    "    \"\"\"\n",
    "    model = find_model_using_name(opt.model)\n",
    "    instance = model(opt)\n",
    "    print(\"model [%s] was created\" % type(instance).__name__)\n",
    "    return instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba3c4073-2168-466c-94f4-6a94ba23464a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m networks\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#import models.CoCosNetworks as CoCosNetworks\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from .base_model import BaseModel\n",
    "from . import networks\n",
    "#import models.CoCosNetworks as CoCosNetworks\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import kornia\n",
    "\n",
    "class Pix2PixModel(BaseModel):\n",
    "    \"\"\" This class implements the pix2pix model, for learning a mapping from input images to output images given paired data.\n",
    "\n",
    "    The model training requires '--dataset_mode aligned' dataset.\n",
    "    By default, it uses a '--netG unet256' U-Net generator,\n",
    "    a '--netD basic' discriminator (PatchGAN),\n",
    "    and a '--gan_mode' vanilla GAN loss (the cross-entropy objective used in the orignal GAN paper).\n",
    "\n",
    "    pix2pix paper: https://arxiv.org/pdf/1611.07004.pdf\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def modify_commandline_options(parser, is_train=True):\n",
    "        \"\"\"Add new dataset-specific options, and rewrite default values for existing options.\n",
    "\n",
    "        Parameters:\n",
    "            parser          -- original option parser\n",
    "            is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.\n",
    "\n",
    "        Returns:\n",
    "            the modified parser.\n",
    "\n",
    "        For pix2pix, we do not use image buffer\n",
    "        The training objective is: GAN Loss + lambda_L1 * ||G(A)-B||_1\n",
    "        By default, we use vanilla GAN loss, UNet with batchnorm, and aligned datasets.\n",
    "        \"\"\"\n",
    "        # changing the default values to match the pix2pix paper (https://phillipi.github.io/pix2pix/)\n",
    "        # parser.set_defaults(norm='batch', netG='unet_256', dataset_mode='aligned')\n",
    "        if is_train:\n",
    "            parser.set_defaults(pool_size=0, gan_mode='vanilla')\n",
    "            parser.add_argument('--lambda_L1', type=float, default=25.0, help='weight for L1 loss')\n",
    "\n",
    "        return parser\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        \"\"\"Initialize the pix2pix class.\n",
    "\n",
    "        Parameters:\n",
    "            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n",
    "        \"\"\"\n",
    "        BaseModel.__init__(self, opt)\n",
    "        # specify the training losses you want to print out. The training/test scripts will call <BaseModel.get_current_losses>\n",
    "        if 'noGAN' in opt.pattern:\n",
    "            self.loss_names = []\n",
    "        else:\n",
    "            if self.opt.netD == 'conv':\n",
    "                self.loss_names = ['G_GAN', 'D']\n",
    "                self.loss_D = 0\n",
    "            else:\n",
    "                self.loss_names = ['G_GAN', 'D_real', 'D_fake']\n",
    "        if 'L1' in opt.pattern:\n",
    "            self.loss_names += ['G_L1']\n",
    "        if 'L2' in opt.pattern:\n",
    "            self.loss_names += ['G_L2']\n",
    "        if 'L3' in opt.pattern:\n",
    "            self.loss_names += ['G_L3']\n",
    "        if 'L4' in opt.pattern:\n",
    "            self.loss_names += ['G_L4']\n",
    "        if 'fft' in opt.pattern:\n",
    "            self.loss_names += ['G_fft']\n",
    "        if 'sobel' in opt.pattern:\n",
    "            self.loss_names += ['G_sobel']\n",
    "        if 'conv' in opt.pattern:\n",
    "            self.loss_names += ['G_conv']\n",
    "        if self.isTrain and ('perc' in opt.pattern or 'contextual' in opt.pattern):\n",
    "            if 'perc' in opt.pattern:\n",
    "                self.loss_names += ['G_perc']\n",
    "                if self.opt.which_perceptual == '5_2':\n",
    "                    self.perceptual_layer = -1\n",
    "                elif self.opt.which_perceptual == '4_2':\n",
    "                    self.perceptual_layer = -2\n",
    "            if 'contextual' in opt.pattern:\n",
    "                self.loss_names += ['G_contextual']\n",
    "            self.vggnet_fix = CoCosNetworks.correspondence.VGG19_feature_color_torchversion(vgg_normal_correct=self.opt.vgg_normal_correct)\n",
    "            self.vggnet_fix.load_state_dict(torch.load('models/vgg19_conv.pth'))\n",
    "            self.vggnet_fix.eval()\n",
    "            for param in self.vggnet_fix.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            self.vggnet_fix.to(self.opt.gpu_ids[0])\n",
    "            self.contextual_forward_loss = CoCosNetworks.ContextualLoss_forward(self.opt)\n",
    "\n",
    "        # specify the images you want to save/display. The training/test scripts will call <BaseModel.get_current_visuals>\n",
    "        self.visual_names = ['real_A', 'fake_B', 'real_B']\n",
    "        # specify the models you want to save to the disk. The training/test scripts will call <BaseModel.save_networks> and <BaseModel.load_networks>\n",
    "        if self.isTrain:\n",
    "            self.model_names = ['G', 'D']\n",
    "        else:  # during test time, only load G\n",
    "            self.model_names = ['G']\n",
    "        # define networks (both generator and discriminator)\n",
    "        self.netG = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, opt.netG, opt.norm,\n",
    "                                      not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids)\n",
    "\n",
    "        if self.isTrain:  # define a discriminator; conditional GANs need to take both input and output images; Therefore, #channels for D is input_nc + output_nc\n",
    "            self.netD = networks.define_D(opt.input_nc + opt.output_nc, opt.ndf, opt.netD,\n",
    "                                          opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain, self.gpu_ids)\n",
    "            if opt.netD == 'conv':\n",
    "                prenet = torch.load('models/vgg19_conv.pth')\n",
    "                netdict = {}\n",
    "                netdict['module.conv1_1.weight'] = prenet['conv1_1.weight']\n",
    "                netdict['module.conv1_1.bias'] = prenet['conv1_1.bias']\n",
    "                self.netD.load_state_dict(netdict)\n",
    "\n",
    "        if self.isTrain:\n",
    "            # define loss functions\n",
    "            self.criterionGAN = networks.GANLoss(opt.gan_mode).to(self.device)\n",
    "            self.criterionL1 = torch.nn.L1Loss()\n",
    "            # initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.\n",
    "            self.optimizer_G = torch.optim.Adam(self.netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "            self.optimizer_D = torch.optim.Adam(self.netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "            self.optimizers.append(self.optimizer_G)\n",
    "            self.optimizers.append(self.optimizer_D)\n",
    "\n",
    "    def set_input(self, input):\n",
    "        \"\"\"Unpack input data from the dataloader and perform necessary pre-processing steps.\n",
    "\n",
    "        Parameters:\n",
    "            input (dict): include the data itself and its metadata information.\n",
    "\n",
    "        The option 'direction' can be used to swap images in domain A and domain B.\n",
    "        \"\"\"\n",
    "        AtoB = self.opt.direction == 'AtoB'\n",
    "        self.real_A = input['A' if AtoB else 'B'].to(self.device)\n",
    "        self.real_B = input['B' if AtoB else 'A'].to(self.device)\n",
    "        self.image_paths = input['A_paths' if AtoB else 'B_paths']\n",
    "        if 'mask' in self.opt.pattern:\n",
    "            self.mask = input['mask'].to(self.device)\n",
    "            self.mask = 0.5 * self.mask + 1.5\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"\n",
    "        self.fake_B = self.netG(self.real_A)  # G(A)\n",
    "\n",
    "    def get_ctx_loss(self, source, target):\n",
    "        contextual_style5_1 = torch.mean(self.contextual_forward_loss(source[-1], target[-1].detach())) * 8\n",
    "        contextual_style4_1 = torch.mean(self.contextual_forward_loss(source[-2], target[-2].detach())) * 4\n",
    "        contextual_style3_1 = torch.mean(self.contextual_forward_loss(F.avg_pool2d(source[-3], 2), F.avg_pool2d(target[-3].detach(), 2))) * 2\n",
    "        if self.opt.use_22ctx:\n",
    "            contextual_style2_1 = torch.mean(self.contextual_forward_loss(F.avg_pool2d(source[-4], 4), F.avg_pool2d(target[-4].detach(), 4))) * 1\n",
    "            return contextual_style5_1 + contextual_style4_1 + contextual_style3_1 + contextual_style2_1\n",
    "        return contextual_style5_1 + contextual_style4_1 + contextual_style3_1\n",
    "\n",
    "\n",
    "    def sobel_conv(self, input):\n",
    "        conv_op = nn.Conv2d(3, 1, 3, bias=False)\n",
    "        sobel_kernel = torch.Tensor([[[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]], \n",
    "                                     [[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]],\n",
    "                                     [[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]]])\n",
    "        sobel_kernel = sobel_kernel.expand((1, 3, 3, 3))\n",
    "        conv_op.weight.data = sobel_kernel\n",
    "        for param in conv_op.parameters():\n",
    "            param.requires_grad = False\n",
    "        conv_op.to(self.opt.gpu_ids[0])\n",
    "        edge_detect = conv_op(input)\n",
    "\n",
    "        conv_hor = nn.Conv2d(3, 1, 3, bias=False)\n",
    "        hor_kernel = torch.Tensor([[[-3, 0, 3], [-10, 0, 10], [-3, 0, 3]], \n",
    "                                   [[-3, 0, 3], [-10, 0, 10], [-3, 0, 3]],\n",
    "                                   [[-3, 0, 3], [-10, 0, 10], [-3, 0, 3]]])\n",
    "        hor_kernel = hor_kernel.expand((1, 3, 3, 3))\n",
    "        conv_hor.weight.data = hor_kernel\n",
    "        for param in conv_hor.parameters():\n",
    "            param.requires_grad = False\n",
    "        conv_hor.to(self.opt.gpu_ids[0])\n",
    "        hor_detect = conv_hor(input)\n",
    "\n",
    "        conv_ver = nn.Conv2d(3, 1, 3, bias=False)\n",
    "        ver_kernel = torch.Tensor([[[-3, -10, -3], [0, 0, 0], [3, 10, 3]], \n",
    "                                   [[-3, -10, -3], [0, 0, 0], [3, 10, 3]],\n",
    "                                   [[-3, -10, -3], [0, 0, 0], [3, 10, 3]]])\n",
    "        ver_kernel = ver_kernel.expand((1, 3, 3, 3))\n",
    "        conv_ver.weight.data = ver_kernel\n",
    "        for param in conv_ver.parameters():\n",
    "            param.requires_grad = False\n",
    "        conv_ver.to(self.opt.gpu_ids[0])\n",
    "        ver_detect = conv_ver(input)\n",
    "\n",
    "        return [edge_detect, hor_detect, ver_detect]\n",
    "    \n",
    "    def frequency_division(self,src_img):\n",
    "        #input:src_img    type:tensor\n",
    "        #output:image_low,image_high    type:tensor\n",
    "        #get low frequency component and high frequency compinent of image\n",
    "        fft_src = torch.rfft( src_img.to(self.opt.gpu_ids[0]), signal_ndim=2, onesided=False ).to(self.opt.gpu_ids[0]) \n",
    "        fft_amp = (fft_src[:,:,:,:,0]**2).to(self.opt.gpu_ids[0]) + (fft_src[:,:,:,:,1]**2).to(self.opt.gpu_ids[0])\n",
    "        fft_amp = torch.sqrt(fft_amp).to(self.opt.gpu_ids[0])\n",
    "        fft_pha = torch.atan2( fft_src[:,:,:,:,1], fft_src[:,:,:,:,0] ).to(self.opt.gpu_ids[0])\n",
    "\n",
    "        # replace the low frequency amplitude part of source with that from target\n",
    "        _, _, h, w = fft_amp.size()\n",
    "        amp_low = torch.zeros(fft_amp.size(), dtype=torch.float).to(self.opt.gpu_ids[0])\n",
    "        b = (  np.floor(np.amin((h,w))*0.1)  ).astype(int)     # get b\n",
    "        amp_low[:,:,0:b,0:b]     = fft_amp[:,:,0:b,0:b]      # top left\n",
    "        amp_low[:,:,0:b,w-b:w]   = fft_amp[:,:,0:b,w-b:w]    # top right\n",
    "        amp_low[:,:,h-b:h,0:b]   = fft_amp[:,:,h-b:h,0:b]    # bottom left\n",
    "        amp_low[:,:,h-b:h,w-b:w] = fft_amp[:,:,h-b:h,w-b:w]  # bottom right\n",
    "        amp_high = fft_amp.to(self.opt.gpu_ids[0]) - amp_low.to(self.opt.gpu_ids[0])\n",
    "\n",
    "        # recompose fft of source\n",
    "        fft_low = torch.zeros( fft_src.size(), dtype=torch.float ).to(self.opt.gpu_ids[0])\n",
    "        fft_high = torch.zeros( fft_src.size(), dtype=torch.float ).to(self.opt.gpu_ids[0])\n",
    "        fft_low[:,:,:,:,0] = torch.cos(fft_pha).to(self.opt.gpu_ids[0]) * amp_low.to(self.opt.gpu_ids[0])\n",
    "        fft_low[:,:,:,:,1] = torch.sin(fft_pha).to(self.opt.gpu_ids[0]) * amp_low.to(self.opt.gpu_ids[0])\n",
    "        fft_high[:,:,:,:,0] = torch.cos(fft_pha).to(self.opt.gpu_ids[0]) * amp_high.to(self.opt.gpu_ids[0])\n",
    "        fft_high[:,:,:,:,1] = torch.sin(fft_pha).to(self.opt.gpu_ids[0]) * amp_high.to(self.opt.gpu_ids[0])\n",
    "\n",
    "        # get the recomposed image: source content, target style\n",
    "        _, _, imgH, imgW = src_img.size()\n",
    "        image_low = torch.irfft(fft_low.to(self.opt.gpu_ids[0]), signal_ndim=2, onesided=False, signal_sizes=[imgH,imgW])\n",
    "        image_high = torch.irfft(fft_high.to(self.opt.gpu_ids[0]), signal_ndim=2, onesided=False, signal_sizes=[imgH,imgW])\n",
    "        return image_low,image_high\n",
    "\n",
    "    def backward_D(self):\n",
    "        \"\"\"Calculate GAN loss for the discriminator\"\"\"\n",
    "        if self.opt.netD == 'conv':\n",
    "            fake_feature = self.netD(self.fake_B)\n",
    "            real_feature = self.netD(self.real_B)\n",
    "            self.loss_D = - self.criterionL1(fake_feature.detach(), real_feature) * self.opt.weight_conv\n",
    "        else:    \n",
    "            # Fake; stop backprop to the generator by detaching fake_B\n",
    "            fake_AB = torch.cat((self.real_A, self.fake_B), 1)  # we use conditional GANs; we need to feed both input and output to the discriminator\n",
    "            pred_fake = self.netD(fake_AB.detach())\n",
    "            self.loss_D_fake = self.criterionGAN(pred_fake, False)\n",
    "            # Real\n",
    "            real_AB = torch.cat((self.real_A, self.real_B), 1)\n",
    "            pred_real = self.netD(real_AB)\n",
    "            self.loss_D_real = self.criterionGAN(pred_real, True)\n",
    "            # combine loss and calculate gradients\n",
    "            self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n",
    "        self.loss_D.backward()\n",
    "\n",
    "    def backward_G(self):\n",
    "        \"\"\"Calculate GAN and L1 loss for the generator\"\"\"\n",
    "        # First, G(A) should fake the discriminator\n",
    "        self.loss_G = 0\n",
    "        if 'noGAN' not in self.opt.pattern:\n",
    "            if self.opt.netD == 'conv':\n",
    "                fake_feature = self.netD(self.fake_B)\n",
    "                real_feature = self.netD(self.real_B)\n",
    "                self.loss_G_GAN = self.criterionL1(fake_feature, real_feature) * self.opt.weight_conv\n",
    "            else:\n",
    "                fake_AB = torch.cat((self.real_A, self.fake_B), 1)\n",
    "                pred_fake = self.netD(fake_AB)\n",
    "                self.loss_G_GAN = self.criterionGAN(pred_fake, True)\n",
    "            self.loss_G += self.loss_G_GAN\n",
    "\n",
    "        if 'L1' in self.opt.pattern:\n",
    "            self.loss_G_L1 = self.criterionL1(self.fake_B, self.real_B) * self.opt.lambda_L1\n",
    "            self.loss_G += self.loss_G_L1\n",
    "            if 'L2' in self.opt.pattern:\n",
    "                octave1_layer2_fake=kornia.filters.gaussian_blur2d(self.fake_B,(3,3),(1,1))\n",
    "                octave1_layer3_fake=kornia.filters.gaussian_blur2d(octave1_layer2_fake,(3,3),(1,1))\n",
    "                octave1_layer4_fake=kornia.filters.gaussian_blur2d(octave1_layer3_fake,(3,3),(1,1))\n",
    "                octave1_layer5_fake=kornia.filters.gaussian_blur2d(octave1_layer4_fake,(3,3),(1,1))\n",
    "                octave2_layer1_fake=kornia.filters.blur_pool2d(octave1_layer5_fake, 1, stride=2)\n",
    "                octave1_layer2_real=kornia.filters.gaussian_blur2d(self.real_B,(3,3),(1,1))\n",
    "                octave1_layer3_real=kornia.filters.gaussian_blur2d(octave1_layer2_real,(3,3),(1,1))\n",
    "                octave1_layer4_real=kornia.filters.gaussian_blur2d(octave1_layer3_real,(3,3),(1,1))\n",
    "                octave1_layer5_real=kornia.filters.gaussian_blur2d(octave1_layer4_real,(3,3),(1,1))\n",
    "                octave2_layer1_real=kornia.filters.blur_pool2d(octave1_layer5_real, 1, stride=2)\n",
    "                self.loss_G_L2 = self.criterionL1(octave2_layer1_fake, octave2_layer1_real) * self.opt.weight_L2\n",
    "                self.loss_G += self.loss_G_L2\n",
    "                if 'L3' in self.opt.pattern:\n",
    "                    octave2_layer2_fake=kornia.filters.gaussian_blur2d(octave2_layer1_fake,(3,3),(1,1))\n",
    "                    octave2_layer3_fake=kornia.filters.gaussian_blur2d(octave2_layer2_fake,(3,3),(1,1))\n",
    "                    octave2_layer4_fake=kornia.filters.gaussian_blur2d(octave2_layer3_fake,(3,3),(1,1))\n",
    "                    octave2_layer5_fake=kornia.filters.gaussian_blur2d(octave2_layer4_fake,(3,3),(1,1))\n",
    "                    octave3_layer1_fake=kornia.filters.blur_pool2d(octave2_layer5_fake, 1, stride=2)\n",
    "                    octave2_layer2_real=kornia.filters.gaussian_blur2d(octave2_layer1_real,(3,3),(1,1))\n",
    "                    octave2_layer3_real=kornia.filters.gaussian_blur2d(octave2_layer2_real,(3,3),(1,1))\n",
    "                    octave2_layer4_real=kornia.filters.gaussian_blur2d(octave2_layer3_real,(3,3),(1,1))\n",
    "                    octave2_layer5_real=kornia.filters.gaussian_blur2d(octave2_layer4_real,(3,3),(1,1))\n",
    "                    octave3_layer1_real=kornia.filters.blur_pool2d(octave2_layer5_real, 1, stride=2)\n",
    "                    self.loss_G_L3 = self.criterionL1(octave3_layer1_fake, octave3_layer1_real) * self.opt.weight_L3\n",
    "                    self.loss_G += self.loss_G_L3\n",
    "                    if 'L4' in self.opt.pattern:\n",
    "                        octave3_layer2_fake=kornia.filters.gaussian_blur2d(octave3_layer1_fake,(3,3),(1,1))\n",
    "                        octave3_layer3_fake=kornia.filters.gaussian_blur2d(octave3_layer2_fake,(3,3),(1,1))\n",
    "                        octave3_layer4_fake=kornia.filters.gaussian_blur2d(octave3_layer3_fake,(3,3),(1,1))\n",
    "                        octave3_layer5_fake=kornia.filters.gaussian_blur2d(octave3_layer4_fake,(3,3),(1,1))\n",
    "                        octave4_layer1_fake=kornia.filters.blur_pool2d(octave3_layer5_fake, 1, stride=2)\n",
    "                        octave3_layer2_real=kornia.filters.gaussian_blur2d(octave3_layer1_real,(3,3),(1,1))\n",
    "                        octave3_layer3_real=kornia.filters.gaussian_blur2d(octave3_layer2_real,(3,3),(1,1))\n",
    "                        octave3_layer4_real=kornia.filters.gaussian_blur2d(octave3_layer3_real,(3,3),(1,1))\n",
    "                        octave3_layer5_real=kornia.filters.gaussian_blur2d(octave3_layer4_real,(3,3),(1,1))\n",
    "                        octave4_layer1_real=kornia.filters.blur_pool2d(octave3_layer5_real, 1, stride=2)\n",
    "                        self.loss_G_L4 = self.criterionL1(octave4_layer1_fake, octave4_layer1_real) * self.opt.weight_L4\n",
    "                        self.loss_G += self.loss_G_L4\n",
    "            if 'mask' in self.opt.pattern:\n",
    "                self.loss_G_L1 = self.criterionL1(self.fake_B * self.mask, self.real_B * self.mask) * self.opt.lambda_L1\n",
    "        \n",
    "\n",
    "        if 'fft' in self.opt.pattern:\n",
    "            prenet = torch.load('models/vgg19_conv.pth')\n",
    "            self.weight1_1 = prenet['conv1_1.weight'].type(torch.FloatTensor).cuda()\n",
    "            #self.weight1_1 = prenet['conv1_1.weight'].type(torch.FloatTensor)\n",
    "            fake_low,fake_high = self.frequency_division(self.fake_B)\n",
    "            real_low,real_high = self.frequency_division(self.real_B)\n",
    "            self.loss_G_fft = self.criterionL1(fake_low.to(self.opt.gpu_ids[0]),real_low.to(self.opt.gpu_ids[0]))*self.opt.weight_low_L1+self.criterionL1(fake_high.to(self.opt.gpu_ids[0]),real_high.to(self.opt.gpu_ids[0]))*self.opt.weight_high_L1\n",
    "            self.loss_G += self.loss_G_fft\n",
    "\n",
    "        if 'perc' in self.opt.pattern or 'contextual' in self.opt.pattern:\n",
    "            real_features = self.vggnet_fix(self.real_B, ['r12', 'r22', 'r32', 'r42', 'r52'], preprocess=True)\n",
    "            fake_features = self.vggnet_fix(self.fake_B, ['r12', 'r22', 'r32', 'r42', 'r52'], preprocess=True)\n",
    "            if 'perc' in self.opt.pattern:\n",
    "                feat_loss = torch.nn.MSELoss()(fake_features[self.perceptual_layer], real_features[self.perceptual_layer].detach())\n",
    "                self.loss_G_perc = feat_loss * self.opt.weight_perceptual\n",
    "                self.loss_G += self.loss_G_perc\n",
    "            if 'contextual' in self.opt.pattern:\n",
    "                self.loss_G_contextual = self.get_ctx_loss(fake_features, real_features) * self.opt.lambda_vgg * self.opt.ctx_w\n",
    "                self.loss_G += self.loss_G_contextual\n",
    "        \n",
    "        if 'conv' in self.opt.pattern:\n",
    "            # real_conv = self.vggnet_fix(self.real_B, ['r11'], preprocess=True)\n",
    "            # fake_conv = self.vggnet_fix(self.fake_B, ['r11'], preprocess=True)\n",
    "            # conv_loss = 0\n",
    "            # for i in range(len(fake_conv)):\n",
    "            #     conv_loss += self.criterionL1(fake_conv[i], real_conv[i].detach())\n",
    "            prenet = torch.load('models/vgg19_conv.pth')\n",
    "            self.weight1_1 = prenet['conv1_1.weight'].type(torch.FloatTensor).cuda()\n",
    "            fake_feature = F.conv2d(self.fake_B, self.weight1_1, padding=1)\n",
    "            real_feature = F.conv2d(self.real_B, self.weight1_1, padding=1)\n",
    "            conv_loss = self.criterionL1(fake_feature, real_feature)\n",
    "            self.loss_G_conv = conv_loss * self.opt.weight_conv\n",
    "            self.loss_G += self.loss_G_conv\n",
    "\n",
    "        if 'sobel' in self.opt.pattern:\n",
    "            real_sobels = self.sobel_conv(self.real_B)\n",
    "            fake_sobels = self.sobel_conv(self.fake_B)\n",
    "            sobel_loss = 0\n",
    "            for i in range(len(fake_sobels)):\n",
    "                sobel_loss += self.criterionL1(fake_sobels[i], real_sobels[i].detach())\n",
    "            self.loss_G_sobel = sobel_loss * self.opt.weight_sobel\n",
    "            self.loss_G += self.loss_G_sobel\n",
    "        \n",
    "        self.loss_G.backward()\n",
    "\n",
    "    def optimize_parameters(self, fixD=False):\n",
    "        self.forward()                   # compute fake images: G(A)\n",
    "        if 'noGAN' not in self.opt.pattern and not fixD:\n",
    "            # update D\n",
    "            self.set_requires_grad(self.netD, True)  # enable backprop for D\n",
    "            self.optimizer_D.zero_grad()     # set D's gradients to zero\n",
    "            self.backward_D()                # calculate gradients for D\n",
    "            self.optimizer_D.step()          # update D's weights\n",
    "        # update G\n",
    "        self.set_requires_grad(self.netD, False)  # D requires no gradients when optimizing G\n",
    "        self.optimizer_G.zero_grad()        # set G's gradients to zero\n",
    "        self.backward_G()                   # calculate graidents for G\n",
    "        self.optimizer_G.step()             # udpate G's weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "477ba31e-6888-4a04-b1b4-e052b3c102ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m networks\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#import models.CoCosNetworks as CoCosNetworks\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from .base_model import BaseModel\n",
    "from . import networks\n",
    "#import models.CoCosNetworks as CoCosNetworks\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import kornia\n",
    "\n",
    "class Pix2PixModel(BaseModel):\n",
    "    \"\"\" This class implements the pix2pix model, for learning a mapping from input images to output images given paired data.\n",
    "\n",
    "    The model training requires '--dataset_mode aligned' dataset.\n",
    "    By default, it uses a '--netG unet256' U-Net generator,\n",
    "    a '--netD basic' discriminator (PatchGAN),\n",
    "    and a '--gan_mode' vanilla GAN loss (the cross-entropy objective used in the orignal GAN paper).\n",
    "\n",
    "    pix2pix paper: https://arxiv.org/pdf/1611.07004.pdf\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def modify_commandline_options(parser, is_train=True):\n",
    "        \"\"\"Add new dataset-specific options, and rewrite default values for existing options.\n",
    "\n",
    "        Parameters:\n",
    "            parser          -- original option parser\n",
    "            is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.\n",
    "\n",
    "        Returns:\n",
    "            the modified parser.\n",
    "\n",
    "        For pix2pix, we do not use image buffer\n",
    "        The training objective is: GAN Loss + lambda_L1 * ||G(A)-B||_1\n",
    "        By default, we use vanilla GAN loss, UNet with batchnorm, and aligned datasets.\n",
    "        \"\"\"\n",
    "        # changing the default values to match the pix2pix paper (https://phillipi.github.io/pix2pix/)\n",
    "        # parser.set_defaults(norm='batch', netG='unet_256', dataset_mode='aligned')\n",
    "        if is_train:\n",
    "            parser.set_defaults(pool_size=0, gan_mode='vanilla')\n",
    "            parser.add_argument('--lambda_L1', type=float, default=25.0, help='weight for L1 loss')\n",
    "\n",
    "        return parser\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        \"\"\"Initialize the pix2pix class.\n",
    "\n",
    "        Parameters:\n",
    "            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n",
    "        \"\"\"\n",
    "        BaseModel.__init__(self, opt)\n",
    "        # specify the training losses you want to print out. The training/test scripts will call <BaseModel.get_current_losses>\n",
    "        if 'noGAN' in opt.pattern:\n",
    "            self.loss_names = []\n",
    "        else:\n",
    "            if self.opt.netD == 'conv':\n",
    "                self.loss_names = ['G_GAN', 'D']\n",
    "                self.loss_D = 0\n",
    "            else:\n",
    "                self.loss_names = ['G_GAN', 'D_real', 'D_fake']\n",
    "        if 'L1' in opt.pattern:\n",
    "            self.loss_names += ['G_L1']\n",
    "        if 'L2' in opt.pattern:\n",
    "            self.loss_names += ['G_L2']\n",
    "        if 'L3' in opt.pattern:\n",
    "            self.loss_names += ['G_L3']\n",
    "        if 'L4' in opt.pattern:\n",
    "            self.loss_names += ['G_L4']\n",
    "        if 'fft' in opt.pattern:\n",
    "            self.loss_names += ['G_fft']\n",
    "        if 'sobel' in opt.pattern:\n",
    "            self.loss_names += ['G_sobel']\n",
    "        if 'conv' in opt.pattern:\n",
    "            self.loss_names += ['G_conv']\n",
    "        if self.isTrain and ('perc' in opt.pattern or 'contextual' in opt.pattern):\n",
    "            if 'perc' in opt.pattern:\n",
    "                self.loss_names += ['G_perc']\n",
    "                if self.opt.which_perceptual == '5_2':\n",
    "                    self.perceptual_layer = -1\n",
    "                elif self.opt.which_perceptual == '4_2':\n",
    "                    self.perceptual_layer = -2\n",
    "            if 'contextual' in opt.pattern:\n",
    "                self.loss_names += ['G_contextual']\n",
    "            self.vggnet_fix = CoCosNetworks.correspondence.VGG19_feature_color_torchversion(vgg_normal_correct=self.opt.vgg_normal_correct)\n",
    "            self.vggnet_fix.load_state_dict(torch.load('models/vgg19_conv.pth'))\n",
    "            self.vggnet_fix.eval()\n",
    "            for param in self.vggnet_fix.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            self.vggnet_fix.to(self.opt.gpu_ids[0])\n",
    "            self.contextual_forward_loss = CoCosNetworks.ContextualLoss_forward(self.opt)\n",
    "\n",
    "        # specify the images you want to save/display. The training/test scripts will call <BaseModel.get_current_visuals>\n",
    "        self.visual_names = ['real_A', 'fake_B', 'real_B']\n",
    "        # specify the models you want to save to the disk. The training/test scripts will call <BaseModel.save_networks> and <BaseModel.load_networks>\n",
    "        if self.isTrain:\n",
    "            self.model_names = ['G', 'D']\n",
    "        else:  # during test time, only load G\n",
    "            self.model_names = ['G']\n",
    "        # define networks (both generator and discriminator)\n",
    "        self.netG = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, opt.netG, opt.norm,\n",
    "                                      not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids)\n",
    "\n",
    "        if self.isTrain:  # define a discriminator; conditional GANs need to take both input and output images; Therefore, #channels for D is input_nc + output_nc\n",
    "            self.netD = networks.define_D(opt.input_nc + opt.output_nc, opt.ndf, opt.netD,\n",
    "                                          opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain, self.gpu_ids)\n",
    "            if opt.netD == 'conv':\n",
    "                prenet = torch.load('models/vgg19_conv.pth')\n",
    "                netdict = {}\n",
    "                netdict['module.conv1_1.weight'] = prenet['conv1_1.weight']\n",
    "                netdict['module.conv1_1.bias'] = prenet['conv1_1.bias']\n",
    "                self.netD.load_state_dict(netdict)\n",
    "\n",
    "        if self.isTrain:\n",
    "            # define loss functions\n",
    "            self.criterionGAN = networks.GANLoss(opt.gan_mode).to(self.device)\n",
    "            self.criterionL1 = torch.nn.L1Loss()\n",
    "            # initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.\n",
    "            self.optimizer_G = torch.optim.Adam(self.netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "            self.optimizer_D = torch.optim.Adam(self.netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "            self.optimizers.append(self.optimizer_G)\n",
    "            self.optimizers.append(self.optimizer_D)\n",
    "\n",
    "    def set_input(self, input):\n",
    "        \"\"\"Unpack input data from the dataloader and perform necessary pre-processing steps.\n",
    "\n",
    "        Parameters:\n",
    "            input (dict): include the data itself and its metadata information.\n",
    "\n",
    "        The option 'direction' can be used to swap images in domain A and domain B.\n",
    "        \"\"\"\n",
    "        AtoB = self.opt.direction == 'AtoB'\n",
    "        self.real_A = input['A' if AtoB else 'B'].to(self.device)\n",
    "        self.real_B = input['B' if AtoB else 'A'].to(self.device)\n",
    "        self.image_paths = input['A_paths' if AtoB else 'B_paths']\n",
    "        if 'mask' in self.opt.pattern:\n",
    "            self.mask = input['mask'].to(self.device)\n",
    "            self.mask = 0.5 * self.mask + 1.5\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"\n",
    "        self.fake_B = self.netG(self.real_A)  # G(A)\n",
    "\n",
    "    def get_ctx_loss(self, source, target):\n",
    "        contextual_style5_1 = torch.mean(self.contextual_forward_loss(source[-1], target[-1].detach())) * 8\n",
    "        contextual_style4_1 = torch.mean(self.contextual_forward_loss(source[-2], target[-2].detach())) * 4\n",
    "        contextual_style3_1 = torch.mean(self.contextual_forward_loss(F.avg_pool2d(source[-3], 2), F.avg_pool2d(target[-3].detach(), 2))) * 2\n",
    "        if self.opt.use_22ctx:\n",
    "            contextual_style2_1 = torch.mean(self.contextual_forward_loss(F.avg_pool2d(source[-4], 4), F.avg_pool2d(target[-4].detach(), 4))) * 1\n",
    "            return contextual_style5_1 + contextual_style4_1 + contextual_style3_1 + contextual_style2_1\n",
    "        return contextual_style5_1 + contextual_style4_1 + contextual_style3_1\n",
    "\n",
    "\n",
    "    def sobel_conv(self, input):\n",
    "        conv_op = nn.Conv2d(3, 1, 3, bias=False)\n",
    "        sobel_kernel = torch.Tensor([[[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]], \n",
    "                                     [[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]],\n",
    "                                     [[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]]])\n",
    "        sobel_kernel = sobel_kernel.expand((1, 3, 3, 3))\n",
    "        conv_op.weight.data = sobel_kernel\n",
    "        for param in conv_op.parameters():\n",
    "            param.requires_grad = False\n",
    "        conv_op.to(self.opt.gpu_ids[0])\n",
    "        edge_detect = conv_op(input)\n",
    "\n",
    "        conv_hor = nn.Conv2d(3, 1, 3, bias=False)\n",
    "        hor_kernel = torch.Tensor([[[-3, 0, 3], [-10, 0, 10], [-3, 0, 3]], \n",
    "                                   [[-3, 0, 3], [-10, 0, 10], [-3, 0, 3]],\n",
    "                                   [[-3, 0, 3], [-10, 0, 10], [-3, 0, 3]]])\n",
    "        hor_kernel = hor_kernel.expand((1, 3, 3, 3))\n",
    "        conv_hor.weight.data = hor_kernel\n",
    "        for param in conv_hor.parameters():\n",
    "            param.requires_grad = False\n",
    "        conv_hor.to(self.opt.gpu_ids[0])\n",
    "        hor_detect = conv_hor(input)\n",
    "\n",
    "        conv_ver = nn.Conv2d(3, 1, 3, bias=False)\n",
    "        ver_kernel = torch.Tensor([[[-3, -10, -3], [0, 0, 0], [3, 10, 3]], \n",
    "                                   [[-3, -10, -3], [0, 0, 0], [3, 10, 3]],\n",
    "                                   [[-3, -10, -3], [0, 0, 0], [3, 10, 3]]])\n",
    "        ver_kernel = ver_kernel.expand((1, 3, 3, 3))\n",
    "        conv_ver.weight.data = ver_kernel\n",
    "        for param in conv_ver.parameters():\n",
    "            param.requires_grad = False\n",
    "        conv_ver.to(self.opt.gpu_ids[0])\n",
    "        ver_detect = conv_ver(input)\n",
    "\n",
    "        return [edge_detect, hor_detect, ver_detect]\n",
    "    \n",
    "    def frequency_division(self,src_img):\n",
    "        #input:src_img    type:tensor\n",
    "        #output:image_low,image_high    type:tensor\n",
    "        #get low frequency component and high frequency compinent of image\n",
    "        fft_src = torch.rfft( src_img.to(self.opt.gpu_ids[0]), signal_ndim=2, onesided=False ).to(self.opt.gpu_ids[0]) \n",
    "        fft_amp = (fft_src[:,:,:,:,0]**2).to(self.opt.gpu_ids[0]) + (fft_src[:,:,:,:,1]**2).to(self.opt.gpu_ids[0])\n",
    "        fft_amp = torch.sqrt(fft_amp).to(self.opt.gpu_ids[0])\n",
    "        fft_pha = torch.atan2( fft_src[:,:,:,:,1], fft_src[:,:,:,:,0] ).to(self.opt.gpu_ids[0])\n",
    "\n",
    "        # replace the low frequency amplitude part of source with that from target\n",
    "        _, _, h, w = fft_amp.size()\n",
    "        amp_low = torch.zeros(fft_amp.size(), dtype=torch.float).to(self.opt.gpu_ids[0])\n",
    "        b = (  np.floor(np.amin((h,w))*0.1)  ).astype(int)     # get b\n",
    "        amp_low[:,:,0:b,0:b]     = fft_amp[:,:,0:b,0:b]      # top left\n",
    "        amp_low[:,:,0:b,w-b:w]   = fft_amp[:,:,0:b,w-b:w]    # top right\n",
    "        amp_low[:,:,h-b:h,0:b]   = fft_amp[:,:,h-b:h,0:b]    # bottom left\n",
    "        amp_low[:,:,h-b:h,w-b:w] = fft_amp[:,:,h-b:h,w-b:w]  # bottom right\n",
    "        amp_high = fft_amp.to(self.opt.gpu_ids[0]) - amp_low.to(self.opt.gpu_ids[0])\n",
    "\n",
    "        # recompose fft of source\n",
    "        fft_low = torch.zeros( fft_src.size(), dtype=torch.float ).to(self.opt.gpu_ids[0])\n",
    "        fft_high = torch.zeros( fft_src.size(), dtype=torch.float ).to(self.opt.gpu_ids[0])\n",
    "        fft_low[:,:,:,:,0] = torch.cos(fft_pha).to(self.opt.gpu_ids[0]) * amp_low.to(self.opt.gpu_ids[0])\n",
    "        fft_low[:,:,:,:,1] = torch.sin(fft_pha).to(self.opt.gpu_ids[0]) * amp_low.to(self.opt.gpu_ids[0])\n",
    "        fft_high[:,:,:,:,0] = torch.cos(fft_pha).to(self.opt.gpu_ids[0]) * amp_high.to(self.opt.gpu_ids[0])\n",
    "        fft_high[:,:,:,:,1] = torch.sin(fft_pha).to(self.opt.gpu_ids[0]) * amp_high.to(self.opt.gpu_ids[0])\n",
    "\n",
    "        # get the recomposed image: source content, target style\n",
    "        _, _, imgH, imgW = src_img.size()\n",
    "        image_low = torch.irfft(fft_low.to(self.opt.gpu_ids[0]), signal_ndim=2, onesided=False, signal_sizes=[imgH,imgW])\n",
    "        image_high = torch.irfft(fft_high.to(self.opt.gpu_ids[0]), signal_ndim=2, onesided=False, signal_sizes=[imgH,imgW])\n",
    "        return image_low,image_high\n",
    "\n",
    "    def backward_D(self):\n",
    "        \"\"\"Calculate GAN loss for the discriminator\"\"\"\n",
    "        if self.opt.netD == 'conv':\n",
    "            fake_feature = self.netD(self.fake_B)\n",
    "            real_feature = self.netD(self.real_B)\n",
    "            self.loss_D = - self.criterionL1(fake_feature.detach(), real_feature) * self.opt.weight_conv\n",
    "        else:    \n",
    "            # Fake; stop backprop to the generator by detaching fake_B\n",
    "            fake_AB = torch.cat((self.real_A, self.fake_B), 1)  # we use conditional GANs; we need to feed both input and output to the discriminator\n",
    "            pred_fake = self.netD(fake_AB.detach())\n",
    "            self.loss_D_fake = self.criterionGAN(pred_fake, False)\n",
    "            # Real\n",
    "            real_AB = torch.cat((self.real_A, self.real_B), 1)\n",
    "            pred_real = self.netD(real_AB)\n",
    "            self.loss_D_real = self.criterionGAN(pred_real, True)\n",
    "            # combine loss and calculate gradients\n",
    "            self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n",
    "        self.loss_D.backward()\n",
    "\n",
    "    def backward_G(self):\n",
    "        \"\"\"Calculate GAN and L1 loss for the generator\"\"\"\n",
    "        # First, G(A) should fake the discriminator\n",
    "        self.loss_G = 0\n",
    "        if 'noGAN' not in self.opt.pattern:\n",
    "            if self.opt.netD == 'conv':\n",
    "                fake_feature = self.netD(self.fake_B)\n",
    "                real_feature = self.netD(self.real_B)\n",
    "                self.loss_G_GAN = self.criterionL1(fake_feature, real_feature) * self.opt.weight_conv\n",
    "            else:\n",
    "                fake_AB = torch.cat((self.real_A, self.fake_B), 1)\n",
    "                pred_fake = self.netD(fake_AB)\n",
    "                self.loss_G_GAN = self.criterionGAN(pred_fake, True)\n",
    "            self.loss_G += self.loss_G_GAN\n",
    "\n",
    "        if 'L1' in self.opt.pattern:\n",
    "            self.loss_G_L1 = self.criterionL1(self.fake_B, self.real_B) * self.opt.lambda_L1\n",
    "            self.loss_G += self.loss_G_L1\n",
    "            if 'L2' in self.opt.pattern:\n",
    "                octave1_layer2_fake=kornia.filters.gaussian_blur2d(self.fake_B,(3,3),(1,1))\n",
    "                octave1_layer3_fake=kornia.filters.gaussian_blur2d(octave1_layer2_fake,(3,3),(1,1))\n",
    "                octave1_layer4_fake=kornia.filters.gaussian_blur2d(octave1_layer3_fake,(3,3),(1,1))\n",
    "                octave1_layer5_fake=kornia.filters.gaussian_blur2d(octave1_layer4_fake,(3,3),(1,1))\n",
    "                octave2_layer1_fake=kornia.filters.blur_pool2d(octave1_layer5_fake, 1, stride=2)\n",
    "                octave1_layer2_real=kornia.filters.gaussian_blur2d(self.real_B,(3,3),(1,1))\n",
    "                octave1_layer3_real=kornia.filters.gaussian_blur2d(octave1_layer2_real,(3,3),(1,1))\n",
    "                octave1_layer4_real=kornia.filters.gaussian_blur2d(octave1_layer3_real,(3,3),(1,1))\n",
    "                octave1_layer5_real=kornia.filters.gaussian_blur2d(octave1_layer4_real,(3,3),(1,1))\n",
    "                octave2_layer1_real=kornia.filters.blur_pool2d(octave1_layer5_real, 1, stride=2)\n",
    "                self.loss_G_L2 = self.criterionL1(octave2_layer1_fake, octave2_layer1_real) * self.opt.weight_L2\n",
    "                self.loss_G += self.loss_G_L2\n",
    "                if 'L3' in self.opt.pattern:\n",
    "                    octave2_layer2_fake=kornia.filters.gaussian_blur2d(octave2_layer1_fake,(3,3),(1,1))\n",
    "                    octave2_layer3_fake=kornia.filters.gaussian_blur2d(octave2_layer2_fake,(3,3),(1,1))\n",
    "                    octave2_layer4_fake=kornia.filters.gaussian_blur2d(octave2_layer3_fake,(3,3),(1,1))\n",
    "                    octave2_layer5_fake=kornia.filters.gaussian_blur2d(octave2_layer4_fake,(3,3),(1,1))\n",
    "                    octave3_layer1_fake=kornia.filters.blur_pool2d(octave2_layer5_fake, 1, stride=2)\n",
    "                    octave2_layer2_real=kornia.filters.gaussian_blur2d(octave2_layer1_real,(3,3),(1,1))\n",
    "                    octave2_layer3_real=kornia.filters.gaussian_blur2d(octave2_layer2_real,(3,3),(1,1))\n",
    "                    octave2_layer4_real=kornia.filters.gaussian_blur2d(octave2_layer3_real,(3,3),(1,1))\n",
    "                    octave2_layer5_real=kornia.filters.gaussian_blur2d(octave2_layer4_real,(3,3),(1,1))\n",
    "                    octave3_layer1_real=kornia.filters.blur_pool2d(octave2_layer5_real, 1, stride=2)\n",
    "                    self.loss_G_L3 = self.criterionL1(octave3_layer1_fake, octave3_layer1_real) * self.opt.weight_L3\n",
    "                    self.loss_G += self.loss_G_L3\n",
    "                    if 'L4' in self.opt.pattern:\n",
    "                        octave3_layer2_fake=kornia.filters.gaussian_blur2d(octave3_layer1_fake,(3,3),(1,1))\n",
    "                        octave3_layer3_fake=kornia.filters.gaussian_blur2d(octave3_layer2_fake,(3,3),(1,1))\n",
    "                        octave3_layer4_fake=kornia.filters.gaussian_blur2d(octave3_layer3_fake,(3,3),(1,1))\n",
    "                        octave3_layer5_fake=kornia.filters.gaussian_blur2d(octave3_layer4_fake,(3,3),(1,1))\n",
    "                        octave4_layer1_fake=kornia.filters.blur_pool2d(octave3_layer5_fake, 1, stride=2)\n",
    "                        octave3_layer2_real=kornia.filters.gaussian_blur2d(octave3_layer1_real,(3,3),(1,1))\n",
    "                        octave3_layer3_real=kornia.filters.gaussian_blur2d(octave3_layer2_real,(3,3),(1,1))\n",
    "                        octave3_layer4_real=kornia.filters.gaussian_blur2d(octave3_layer3_real,(3,3),(1,1))\n",
    "                        octave3_layer5_real=kornia.filters.gaussian_blur2d(octave3_layer4_real,(3,3),(1,1))\n",
    "                        octave4_layer1_real=kornia.filters.blur_pool2d(octave3_layer5_real, 1, stride=2)\n",
    "                        self.loss_G_L4 = self.criterionL1(octave4_layer1_fake, octave4_layer1_real) * self.opt.weight_L4\n",
    "                        self.loss_G += self.loss_G_L4\n",
    "            if 'mask' in self.opt.pattern:\n",
    "                self.loss_G_L1 = self.criterionL1(self.fake_B * self.mask, self.real_B * self.mask) * self.opt.lambda_L1\n",
    "        \n",
    "\n",
    "        if 'fft' in self.opt.pattern:\n",
    "            prenet = torch.load('models/vgg19_conv.pth')\n",
    "            self.weight1_1 = prenet['conv1_1.weight'].type(torch.FloatTensor).cuda()\n",
    "            #self.weight1_1 = prenet['conv1_1.weight'].type(torch.FloatTensor)\n",
    "            fake_low,fake_high = self.frequency_division(self.fake_B)\n",
    "            real_low,real_high = self.frequency_division(self.real_B)\n",
    "            self.loss_G_fft = self.criterionL1(fake_low.to(self.opt.gpu_ids[0]),real_low.to(self.opt.gpu_ids[0]))*self.opt.weight_low_L1+self.criterionL1(fake_high.to(self.opt.gpu_ids[0]),real_high.to(self.opt.gpu_ids[0]))*self.opt.weight_high_L1\n",
    "            self.loss_G += self.loss_G_fft\n",
    "\n",
    "        if 'perc' in self.opt.pattern or 'contextual' in self.opt.pattern:\n",
    "            real_features = self.vggnet_fix(self.real_B, ['r12', 'r22', 'r32', 'r42', 'r52'], preprocess=True)\n",
    "            fake_features = self.vggnet_fix(self.fake_B, ['r12', 'r22', 'r32', 'r42', 'r52'], preprocess=True)\n",
    "            if 'perc' in self.opt.pattern:\n",
    "                feat_loss = torch.nn.MSELoss()(fake_features[self.perceptual_layer], real_features[self.perceptual_layer].detach())\n",
    "                self.loss_G_perc = feat_loss * self.opt.weight_perceptual\n",
    "                self.loss_G += self.loss_G_perc\n",
    "            if 'contextual' in self.opt.pattern:\n",
    "                self.loss_G_contextual = self.get_ctx_loss(fake_features, real_features) * self.opt.lambda_vgg * self.opt.ctx_w\n",
    "                self.loss_G += self.loss_G_contextual\n",
    "        \n",
    "        if 'conv' in self.opt.pattern:\n",
    "            # real_conv = self.vggnet_fix(self.real_B, ['r11'], preprocess=True)\n",
    "            # fake_conv = self.vggnet_fix(self.fake_B, ['r11'], preprocess=True)\n",
    "            # conv_loss = 0\n",
    "            # for i in range(len(fake_conv)):\n",
    "            #     conv_loss += self.criterionL1(fake_conv[i], real_conv[i].detach())\n",
    "            prenet = torch.load('models/vgg19_conv.pth')\n",
    "            self.weight1_1 = prenet['conv1_1.weight'].type(torch.FloatTensor).cuda()\n",
    "            fake_feature = F.conv2d(self.fake_B, self.weight1_1, padding=1)\n",
    "            real_feature = F.conv2d(self.real_B, self.weight1_1, padding=1)\n",
    "            conv_loss = self.criterionL1(fake_feature, real_feature)\n",
    "            self.loss_G_conv = conv_loss * self.opt.weight_conv\n",
    "            self.loss_G += self.loss_G_conv\n",
    "\n",
    "        if 'sobel' in self.opt.pattern:\n",
    "            real_sobels = self.sobel_conv(self.real_B)\n",
    "            fake_sobels = self.sobel_conv(self.fake_B)\n",
    "            sobel_loss = 0\n",
    "            for i in range(len(fake_sobels)):\n",
    "                sobel_loss += self.criterionL1(fake_sobels[i], real_sobels[i].detach())\n",
    "            self.loss_G_sobel = sobel_loss * self.opt.weight_sobel\n",
    "            self.loss_G += self.loss_G_sobel\n",
    "        \n",
    "        self.loss_G.backward()\n",
    "\n",
    "    def optimize_parameters(self, fixD=False):\n",
    "        self.forward()                   # compute fake images: G(A)\n",
    "        if 'noGAN' not in self.opt.pattern and not fixD:\n",
    "            # update D\n",
    "            self.set_requires_grad(self.netD, True)  # enable backprop for D\n",
    "            self.optimizer_D.zero_grad()     # set D's gradients to zero\n",
    "            self.backward_D()                # calculate gradients for D\n",
    "            self.optimizer_D.step()          # update D's weights\n",
    "        # update G\n",
    "        self.set_requires_grad(self.netD, False)  # D requires no gradients when optimizing G\n",
    "        self.optimizer_G.zero_grad()        # set G's gradients to zero\n",
    "        self.backward_G()                   # calculate graidents for G\n",
    "        self.optimizer_G.step()             # udpate G's weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112a25b4-879f-476e-9f71-dfd5bb9bd961",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
